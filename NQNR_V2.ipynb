{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtSKLcNPceDt"
   },
   "source": [
    "#Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "w6a-Y-DIX89P"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm import tqdm as tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJKTSEDft1RQ"
   },
   "source": [
    "#Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PvRAaJ-P7wpC"
   },
   "outputs": [],
   "source": [
    "!unzip -q MINDsmall_train.zip -d train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 790,
     "status": "ok",
     "timestamp": 1746315876222,
     "user": {
      "displayName": "Zannatun Sristy",
      "userId": "17694043809177177827"
     },
     "user_tz": 300
    },
    "id": "wMDwdpanYxli",
    "outputId": "54e21061-1f4f-4a11-9d3d-63e8403c67f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51282, 8)\n"
     ]
    }
   ],
   "source": [
    "train_news_path = os.path.abspath('train/news.tsv')\n",
    "Train_News_data=pd.read_table(train_news_path,\n",
    "              header=None,\n",
    "              names=[\n",
    "                  'id', 'category', 'subcategory', 'title', 'abstract', 'url',\n",
    "                  'title_entities', 'abstract_entities'\n",
    "              ])\n",
    "\n",
    "print(Train_News_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1499,
     "status": "ok",
     "timestamp": 1746315877719,
     "user": {
      "displayName": "Zannatun Sristy",
      "userId": "17694043809177177827"
     },
     "user_tz": 300
    },
    "id": "ursaKx2xwEC_",
    "outputId": "0318f043-a9d5-4157-83e3-f49e2204edea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156965, 5)\n"
     ]
    }
   ],
   "source": [
    "train_behaviors_path = os.path.abspath('train/behaviors.tsv')\n",
    "Train_Behaviors_data=pd.read_table(train_behaviors_path,\n",
    "              header=None,\n",
    "              names=[\n",
    "                  'impression_id', 'user_id', 'time', 'history', 'impressions'\n",
    "              ])\n",
    "print(Train_Behaviors_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "924ZSPqZt58-"
   },
   "source": [
    "#Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uHCiohg5t_LZ"
   },
   "outputs": [],
   "source": [
    "!unzip -q MINDsmall_dev.zip -d dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 975,
     "status": "ok",
     "timestamp": 1746315880021,
     "user": {
      "displayName": "Zannatun Sristy",
      "userId": "17694043809177177827"
     },
     "user_tz": 300
    },
    "id": "r0fa5MvOuI7R",
    "outputId": "ebdf0a5f-3e24-4b8c-da05-0654935d9578"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42416, 8)\n"
     ]
    }
   ],
   "source": [
    "val_news_path = os.path.abspath('dev/news.tsv')\n",
    "Val_News_data=pd.read_table(val_news_path,\n",
    "              header=None,\n",
    "              names=[\n",
    "                  'id', 'category', 'subcategory', 'title', 'abstract', 'url',\n",
    "                  'title_entities', 'abstract_entities'\n",
    "              ])\n",
    "print(Val_News_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1017,
     "status": "ok",
     "timestamp": 1746315881037,
     "user": {
      "displayName": "Zannatun Sristy",
      "userId": "17694043809177177827"
     },
     "user_tz": 300
    },
    "id": "DV7EZ0kyv9jh",
    "outputId": "4b601eb6-9230-47f3-e029-48a3d145ca0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73152, 5)\n"
     ]
    }
   ],
   "source": [
    "val_behaviors_path = os.path.abspath('dev/behaviors.tsv')\n",
    "Val_Behaviors_data=pd.read_table(val_behaviors_path,\n",
    "              header=None,\n",
    "              names=[\n",
    "                  'impression_id', 'user_id', 'time', 'history', 'impressions'\n",
    "              ])\n",
    "print(Val_Behaviors_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pia48VKq7OUf"
   },
   "source": [
    "#Subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "I-hTmBn0cOkg"
   },
   "outputs": [],
   "source": [
    "def subsample_train_val_same_users_split_by_source(\n",
    "    Train_News_data, Train_Behaviors_data,\n",
    "    Val_News_data, Val_Behaviors_data,\n",
    "    n_users=10000, train_ratio=2.1, seed=42\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    train_users = set(Train_Behaviors_data['user_id'].dropna())\n",
    "    val_users = set(Val_Behaviors_data['user_id'].dropna())\n",
    "    common_users = np.array(list(train_users & val_users))\n",
    "    common_user_ratio = len(train_users) / len(common_users)\n",
    "\n",
    "    common_user_count = min(int(n_users / common_user_ratio), len(common_users))\n",
    "    noncommon_user_count = n_users - common_user_count\n",
    "\n",
    "    sampled_common_users = rng.choice(common_users, size=common_user_count, replace=False)\n",
    "\n",
    "    train_unique_users = np.setdiff1d(list(train_users), common_users)\n",
    "    val_unique_users = np.setdiff1d(list(val_users), common_users)\n",
    "\n",
    "    sampled_train_unique = rng.choice(train_unique_users, size=noncommon_user_count, replace=False)\n",
    "    sampled_val_unique = rng.choice(val_unique_users, size=noncommon_user_count, replace=False)\n",
    "\n",
    "    user_train_logs = pd.concat([\n",
    "        Train_Behaviors_data[Train_Behaviors_data['user_id'].isin(sampled_common_users)],\n",
    "        Train_Behaviors_data[Train_Behaviors_data['user_id'].isin(sampled_train_unique)]\n",
    "    ])\n",
    "\n",
    "    user_val_logs = pd.concat([\n",
    "        Val_Behaviors_data[Val_Behaviors_data['user_id'].isin(sampled_common_users)],\n",
    "        Val_Behaviors_data[Val_Behaviors_data['user_id'].isin(sampled_val_unique)]\n",
    "    ])\n",
    "\n",
    "    target_val_size = int(len(user_train_logs) / train_ratio)\n",
    "    if target_val_size < len(user_val_logs):\n",
    "        user_val_logs = user_val_logs.sample(n=target_val_size, random_state=seed)\n",
    "\n",
    "    def get_referenced_news(news_df, behaviors_df):\n",
    "        news_ids = set()\n",
    "        for _, row in behaviors_df.iterrows():\n",
    "            history = str(row['history']) if not pd.isna(row['history']) else ''\n",
    "            news_ids.update(history.split())\n",
    "            impressions = str(row['impressions']) if not pd.isna(row['impressions']) else ''\n",
    "            news_ids.update(x.split('-')[0] for x in impressions.split())\n",
    "        return news_df[news_df['id'].astype(str).isin(news_ids)].copy()\n",
    "\n",
    "    train_news = get_referenced_news(Train_News_data, user_train_logs)\n",
    "    val_news = get_referenced_news(Val_News_data, user_val_logs)\n",
    "\n",
    "    return train_news, user_train_logs, val_news, user_val_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 256,
     "status": "ok",
     "timestamp": 1746319932775,
     "user": {
      "displayName": "Zannatun Sristy",
      "userId": "17694043809177177827"
     },
     "user_tz": 300
    },
    "id": "94L10-wxcTrv",
    "outputId": "06cb2684-355a-4814-a0ae-03385a679c5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train News Data Shape: (23705, 8)\n",
      "Train Behaviors Data Shape: (15667, 5)\n",
      "Valid News Data Shape: (18666, 8)\n",
      "Valid Behaviors Data Shape: (7369, 5)\n"
     ]
    }
   ],
   "source": [
    "# Subsample data\n",
    "Train_News_data, Train_Behaviors_data, Val_News_data, Val_Behaviors_data = subsample_train_val_same_users_split_by_source(Train_News_data, Train_Behaviors_data, Val_News_data, Val_Behaviors_data, n_users=5000,  train_ratio=2.1)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Train News Data Shape: {Train_News_data.shape}\")\n",
    "print(f\"Train Behaviors Data Shape: {Train_Behaviors_data.shape}\")\n",
    "print(f\"Valid News Data Shape: {Val_News_data.shape}\")\n",
    "print(f\"Valid Behaviors Data Shape: {Val_Behaviors_data.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFc6hZfSvvmB"
   },
   "source": [
    "#Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 497559,
     "status": "ok",
     "timestamp": 1746316380644,
     "user": {
      "displayName": "Zannatun Sristy",
      "userId": "17694043809177177827"
     },
     "user_tz": 300
    },
    "id": "iKuZ7j2n4geD",
    "outputId": "779805e3-719f-41e6-fdbf-a2b7ff43f1e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-05-03 23:44:43--  https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip [following]\n",
      "--2025-05-03 23:44:43--  https://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2176768927 (2.0G) [application/zip]\n",
      "Saving to: ‘glove.840B.300d.zip’\n",
      "\n",
      "glove.840B.300d.zip 100%[===================>]   2.03G  5.02MB/s    in 6m 50s  \n",
      "\n",
      "2025-05-03 23:51:34 (5.06 MB/s) - ‘glove.840B.300d.zip’ saved [2176768927/2176768927]\n",
      "\n",
      "Archive:  glove.840B.300d.zip\n",
      "  inflating: glove.840B.300d.txt     \n"
     ]
    }
   ],
   "source": [
    "!wget https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "!unzip glove.840B.300d.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "Im58Wfw-xC4j"
   },
   "outputs": [],
   "source": [
    "def build_vocab(df, glove=None, min_freq=1):\n",
    "    counter = Counter()\n",
    "\n",
    "    for text in df['title'].fillna('').tolist() + df['abstract'].fillna('').tolist():\n",
    "        tokens = text.lower().split()\n",
    "        counter.update(tokens)\n",
    "\n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    oov_words = 0\n",
    "    in_glove_words = 0\n",
    "\n",
    "    for word, freq in counter.items():\n",
    "        if freq >= min_freq:\n",
    "            if glove is None or word in glove:\n",
    "                vocab[word] = len(vocab)\n",
    "                in_glove_words += 1\n",
    "            else:\n",
    "                vocab[word] = len(vocab)\n",
    "                oov_words += 1\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "6sF76tH5pZ_o"
   },
   "outputs": [],
   "source": [
    "def build_category_indices(df):\n",
    "    cat2idx = {cat: idx for idx, cat in enumerate(df['category'].dropna().unique())}\n",
    "    subcat2idx = {subcat: idx for idx, subcat in enumerate(df['subcategory'].dropna().unique())}\n",
    "    return cat2idx, subcat2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "-sUp8Ukqvz3n"
   },
   "outputs": [],
   "source": [
    "def tokenize(text, vocab, max_len=20):\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    tokens = text.lower().split()\n",
    "    token_ids = [vocab.get(w, vocab['<UNK>']) for w in tokens[:max_len]]\n",
    "    token_ids += [vocab['<PAD>']] * (max_len - len(token_ids))\n",
    "    return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "1xws_dBk9FhO"
   },
   "outputs": [],
   "source": [
    "def load_glove_embeddings(glove_path, vocab=None, embed_dim=300):\n",
    "    glove = {}\n",
    "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f):\n",
    "            values = line.strip().split()\n",
    "            if len(values) != embed_dim + 1:\n",
    "                continue\n",
    "            word = values[0]\n",
    "\n",
    "            if vocab is not None and word not in vocab:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                vector = torch.tensor([float(v) for v in values[1:]], dtype=torch.float32)\n",
    "                glove[word] = vector\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    return glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Us1JcN6uI0_7"
   },
   "source": [
    "#Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "dqT9Zfx-I8sU"
   },
   "outputs": [],
   "source": [
    "def clicked_candidate_news_preparation(behaviors, news):\n",
    "    history_field = behaviors['history']\n",
    "    impressions_field = behaviors['impressions']\n",
    "\n",
    "    history_ids = history_field.split() if isinstance(history_field, str) and history_field.strip() else []\n",
    "    impression_pairs = impressions_field.split() if isinstance(impressions_field, str) and impressions_field.strip() else []\n",
    "\n",
    "    candidate_ids = [impr.split('-')[0] for impr in impression_pairs]\n",
    "    candidate_labels = [int(impr.split('-')[1]) for impr in impression_pairs]\n",
    "\n",
    "    clicked_news = [nid for nid in history_ids if nid in set(news['id'])]\n",
    "    candidate_news = [nid for nid in candidate_ids if nid in set(news['id'])]\n",
    "\n",
    "    return clicked_news, candidate_news, candidate_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "FVZ5oBzl-4MO"
   },
   "outputs": [],
   "source": [
    "def encode(title, abstract, category, subcategory, vocab, cat2idx, subcat2idx, max_len=20):\n",
    "    title = tokenize(title, vocab, max_len)\n",
    "    abstract = tokenize(abstract, vocab, max_len)\n",
    "    category = cat2idx.get(category, 0)\n",
    "    subcategory = subcat2idx.get(subcategory, 0)\n",
    "\n",
    "    title_tensor = torch.tensor(title, dtype=torch.long)\n",
    "    abstract_tensor = torch.tensor(abstract, dtype=torch.long)\n",
    "    cat_tensor = torch.tensor(category, dtype=torch.long)\n",
    "    subcat_tensor = torch.tensor(subcategory, dtype=torch.long)\n",
    "    return title_tensor, abstract_tensor, cat_tensor, subcat_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "WQH2pFigA92B"
   },
   "outputs": [],
   "source": [
    "def build_news_vec_dict(News_data, news_encoder, vocab, cat2idx, subcat2idx, device):\n",
    "    news_vec_dict = {}\n",
    "    news_encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, row in tqdm(News_data.iterrows(), total=len(News_data)):\n",
    "            title_tensor, abstract_tensor, cat_tensor, subcat_tensor = encode(\n",
    "                row['title'], row['abstract'], row['category'], row['subcategory'],\n",
    "                vocab, cat2idx, subcat2idx\n",
    "            )\n",
    "\n",
    "            title_tensor = title_tensor.unsqueeze(0).to(device)\n",
    "            abstract_tensor = abstract_tensor.unsqueeze(0).to(device)\n",
    "            cat_tensor = cat_tensor.unsqueeze(0).to(device)\n",
    "            subcat_tensor = subcat_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "            final_vec, rt, ra = news_encoder(title_tensor, abstract_tensor, cat_tensor, subcat_tensor)\n",
    "\n",
    "            news_vec_dict[row['id']] = (\n",
    "                final_vec.squeeze(0).cpu(),\n",
    "                rt.squeeze(0).cpu(),\n",
    "                ra.squeeze(0).cpu()\n",
    "            )\n",
    "    return news_vec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_news_vec_dict_to_device(news_vec_dict, device):\n",
    "    for nid in news_vec_dict:\n",
    "        news_vec_dict[nid] = tuple(t.to(device) for t in news_vec_dict[nid])\n",
    "    return news_vec_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HsrmmTNcqPY"
   },
   "source": [
    "#Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wzjz8O5cdCZ5"
   },
   "source": [
    "##Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "TbdXAh8vc_8O"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bi1HgGSEcyvu"
   },
   "source": [
    "##News Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ccezVPD1ZSdZ"
   },
   "outputs": [],
   "source": [
    "class NewsEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, label_size):\n",
    "        super(NewsEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.conv_groups = nn.ModuleList([\n",
    "            nn.Conv1d(embed_dim, embed_dim, kernel_size=3, padding=1) for _ in range(3)\n",
    "        ])\n",
    "\n",
    "        self.reduce_proj = nn.Linear(embed_dim * 3, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, embed_dim // 2, bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.title_att_W = nn.Linear(embed_dim, embed_dim)\n",
    "        self.title_att_b = nn.Parameter(torch.zeros(embed_dim))\n",
    "        self.title_att_v = nn.Linear(embed_dim, 1)\n",
    "\n",
    "        self.label_embed = nn.Embedding(label_size, embed_dim)\n",
    "        self.label_fc = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self.fusion_q = nn.Parameter(torch.randn(embed_dim))\n",
    "        self.fusion_Uv = nn.Linear(embed_dim, embed_dim)\n",
    "        self.fusion_uv = nn.Parameter(torch.zeros(embed_dim))\n",
    "\n",
    "    def text_encoder(self, text):\n",
    "        B = self.embedding(text).transpose(1, 2)\n",
    "        group_outputs = [conv(B) for conv in self.conv_groups]\n",
    "        B = F.relu(torch.cat(group_outputs, dim=1)).transpose(1, 2)\n",
    "        B = self.reduce_proj(B)\n",
    "        B, _ = self.lstm(B)\n",
    "        att = torch.tanh(self.title_att_W(B) + self.title_att_b)\n",
    "        alpha = F.softmax(self.title_att_v(att), dim=1)\n",
    "        return torch.sum(alpha * B, dim=1)\n",
    "\n",
    "    def label_encoder(self, x):\n",
    "        ex = self.label_embed(x)\n",
    "        return self.label_fc(ex)\n",
    "\n",
    "    def forward(self, title, abstract, category, subcategory):\n",
    "        rt = self.text_encoder(title)\n",
    "        ra = self.text_encoder(abstract)\n",
    "\n",
    "        rc = self.label_encoder(category)\n",
    "        rsc = self.label_encoder(subcategory)\n",
    "\n",
    "        at = torch.matmul(torch.tanh(self.fusion_Uv(rt) + self.fusion_uv), self.fusion_q)\n",
    "        aa = torch.matmul(torch.tanh(self.fusion_Uv(ra) + self.fusion_uv), self.fusion_q)\n",
    "        ac = torch.matmul(torch.tanh(self.fusion_Uv(rc) + self.fusion_uv), self.fusion_q)\n",
    "        asc = torch.matmul(torch.tanh(self.fusion_Uv(rsc) + self.fusion_uv), self.fusion_q)\n",
    "\n",
    "        weights = torch.stack([at, aa, ac, asc], dim=1)\n",
    "        alphas = F.softmax(weights, dim=1)\n",
    "        final_rep = (\n",
    "              alphas[:, 0].unsqueeze(1) * rt +\n",
    "              alphas[:, 1].unsqueeze(1) * ra +\n",
    "              alphas[:, 2].unsqueeze(1) * rc +\n",
    "              alphas[:, 3].unsqueeze(1) * rsc\n",
    "        )\n",
    "        return final_rep, rt, ra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlNzo0_M-NUp"
   },
   "source": [
    "##Detection Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "FsC5e0aKtlsl"
   },
   "outputs": [],
   "source": [
    "class DetectionModule(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, threshold=0.49, lambda_weight=0.8):\n",
    "        super(DetectionModule, self).__init__()\n",
    "        self.multihead_attention = nn.MultiheadAttention(embed_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.threshold = threshold\n",
    "        self.lambda_weight = lambda_weight\n",
    "\n",
    "\n",
    "    def forward(self, rt, ra):\n",
    "        rt = rt.unsqueeze(0)\n",
    "        ra = ra.unsqueeze(0)\n",
    "        tilda_rt,_ = self.multihead_attention(query=ra, key=rt, value=rt)\n",
    "\n",
    "        tilda_rt_norm = F.normalize(tilda_rt, p=2, dim=-1)\n",
    "        ra_norm = F.normalize(ra, p=2, dim=-1)\n",
    "\n",
    "        cos_sim = (tilda_rt_norm * ra_norm).sum(dim=-1)\n",
    "        pi = torch.sigmoid(cos_sim)\n",
    "\n",
    "        Si =  Si = torch.where(pi > self.threshold,\n",
    "                     torch.full_like(pi, self.lambda_weight),\n",
    "                     torch.ones_like(pi))\n",
    "        return Si.squeeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3YTOKDjLeZb"
   },
   "source": [
    "##User Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "51CaymkCLcuC"
   },
   "outputs": [],
   "source": [
    "class UserEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(UserEncoder, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.q_linear = nn.ModuleList([nn.Linear(embed_dim, embed_dim) for _ in range(num_heads)])\n",
    "        self.v_linear = nn.ModuleList([nn.Linear(embed_dim, embed_dim) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, history_vecs, candidate_vec, detection_score=None):\n",
    "        B, N, D = history_vecs.shape\n",
    "\n",
    "        head_outputs = []\n",
    "\n",
    "        for k in range(self.num_heads):\n",
    "            qk = self.q_linear[k](history_vecs)\n",
    "            scores = torch.matmul(qk, history_vecs.transpose(1, 2)) / (D ** 0.5)\n",
    "            beta = F.softmax(scores, dim=-1)\n",
    "\n",
    "            vk = self.v_linear[k](history_vecs)\n",
    "            head_output = torch.matmul(beta, vk)\n",
    "            head_outputs.append(head_output)\n",
    "\n",
    "        multihead_output = torch.cat(head_outputs, dim=-1)\n",
    "        multihead_output = multihead_output.view(B, N, self.num_heads, D).mean(dim=2)\n",
    "\n",
    "        candidate_vec = candidate_vec.unsqueeze(1)\n",
    "        dot_scores = (candidate_vec * multihead_output).sum(dim=2) / (D ** 0.5)\n",
    "\n",
    "        if detection_score is not None:\n",
    "            dot_scores = dot_scores * detection_score\n",
    "\n",
    "        alpha = F.softmax(dot_scores, dim=1)\n",
    "        user_vector = torch.bmm(alpha.unsqueeze(1), multihead_output).squeeze(1)\n",
    "        return user_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67Lkd6PMDG1q"
   },
   "source": [
    "#Score Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "-JXalL7PxVrB"
   },
   "outputs": [],
   "source": [
    "def compute_click_scores(user_vec, candidate_vecs, detection_scores):\n",
    "    B = candidate_vecs.shape[0]\n",
    "    user_vec_expand = user_vec.expand(B, -1)\n",
    "    dot_scores = (user_vec_expand * candidate_vecs).sum(dim=1)\n",
    "    dd_scores = dot_scores * detection_scores\n",
    "    click_scores = torch.sigmoid(dd_scores)\n",
    "\n",
    "\n",
    "    return click_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8oteKukBdi7a"
   },
   "source": [
    "#Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "5CI4nnb_dc5u"
   },
   "outputs": [],
   "source": [
    "def ranking_softmax_loss(pos_scores, neg_scores):\n",
    "    pos_exp = torch.exp(-pos_scores)\n",
    "    neg_exp = torch.exp(-neg_scores).sum(dim=1)\n",
    "    denom = pos_exp + neg_exp\n",
    "    loss = -torch.log(pos_exp / denom)\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xofUHpwHq9T"
   },
   "source": [
    "#Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "enbeVBeeIK8V"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "TSeicvuMWr4z"
   },
   "outputs": [],
   "source": [
    "def compute_mrr(y_true_sorted):\n",
    "    y_true_sorted = np.asarray(y_true_sorted)\n",
    "    relevant_indices = np.where(y_true_sorted == 1)[0]\n",
    "\n",
    "    if relevant_indices.size == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return 1.0 / (relevant_indices[0] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "oDwRUPfCHw4c"
   },
   "outputs": [],
   "source": [
    "def ndcg(y_true_sorted, k):\n",
    "    y_true_sorted = np.asarray(y_true_sorted)\n",
    "    k = min(k, len(y_true_sorted))\n",
    "    if k == 0:\n",
    "        return 0.0\n",
    "\n",
    "    gains = 2 ** y_true_sorted[:k] - 1\n",
    "    discounts = np.log2(np.arange(2, k + 2))\n",
    "    dcg = np.sum(gains / discounts)\n",
    "\n",
    "    ideal_sorted = np.sort(y_true_sorted)[::-1]\n",
    "    ideal_gains = 2 ** ideal_sorted[:k] - 1\n",
    "    ideal_dcg = np.sum(ideal_gains / discounts)\n",
    "\n",
    "    return dcg / ideal_dcg if ideal_dcg > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "_9l9CWpsITtZ"
   },
   "outputs": [],
   "source": [
    "def eval_metrics(y_true, y_score):\n",
    "    aucs, mrrs, ndcg5s, ndcg10s = [], [], [], []\n",
    "\n",
    "    for labels, scores in zip(y_true, y_score):\n",
    "        sorted_idx = torch.argsort(scores, descending=True)\n",
    "        sorted_labels = labels[sorted_idx]\n",
    "\n",
    "        auc = roc_auc_score(labels.numpy(), scores.numpy()) if len(set(labels.numpy())) > 1 else 0.0\n",
    "        mrr = compute_mrr(sorted_labels)\n",
    "        ndcg5 = ndcg(sorted_labels, 5)\n",
    "        ndcg10 = ndcg(sorted_labels, 10)\n",
    "\n",
    "        aucs.append(auc)\n",
    "        mrrs.append(mrr)\n",
    "        ndcg5s.append(ndcg5)\n",
    "        ndcg10s.append(ndcg10)\n",
    "\n",
    "    auc = sum(aucs) / len(aucs)\n",
    "    mrr = sum(mrrs) / len(mrrs)\n",
    "    ndcg5 = sum(ndcg5s) / len(ndcg5s)\n",
    "    ndcg10 = sum(ndcg10s) / len(ndcg10s)\n",
    "\n",
    "    return auc, mrr, ndcg5, ndcg10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93jlgWByllIO"
   },
   "source": [
    "#Model Object Initiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 272740,
     "status": "ok",
     "timestamp": 1746316660652,
     "user": {
      "displayName": "Zannatun Sristy",
      "userId": "17694043809177177827"
     },
     "user_tz": 300
    },
    "id": "7nKwbUheSgwx",
    "outputId": "98044b4a-4eb3-4950-aa67-520bd6dfdbdc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [01:50, 19791.06it/s]\n"
     ]
    }
   ],
   "source": [
    "glove = load_glove_embeddings('glove.840B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 439,
     "status": "ok",
     "timestamp": 1746319962885,
     "user": {
      "displayName": "Zannatun Sristy",
      "userId": "17694043809177177827"
     },
     "user_tz": 300
    },
    "id": "xoPjdM-Cq98d",
    "outputId": "fed25be6-ffad-406e-f3e7-68c4b92488aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Vocab Size: 78876\n",
      "Val Vocab Size: 66231\n"
     ]
    }
   ],
   "source": [
    "vocab_train = build_vocab(Train_News_data, glove)\n",
    "cat2idx_train, subcat2idx_train = build_category_indices(Train_News_data)\n",
    "\n",
    "\n",
    "vocab_val = build_vocab(Val_News_data, glove)\n",
    "cat2idx_val, subcat2idx_val = build_category_indices(Val_News_data)\n",
    "\n",
    "print(f\"Train Vocab Size: {len(vocab_train)}\")\n",
    "print(f\"Val Vocab Size: {len(vocab_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "7h2dgrw1rYhy"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab_train)\n",
    "embed_dim = 300\n",
    "label_size = max(len(cat2idx_train), len(subcat2idx_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "TfVswNnelhxb"
   },
   "outputs": [],
   "source": [
    "news_encoder = NewsEncoder(vocab_size, embed_dim, label_size)\n",
    "user_encoder = UserEncoder(embed_dim=300, num_heads=6)\n",
    "detection_module = DetectionModule(embed_dim=300, num_heads=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "ApAYK_-lDYh0"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1746319972257,
     "user": {
      "displayName": "Zannatun Sristy",
      "userId": "17694043809177177827"
     },
     "user_tz": 300
    },
    "id": "sc9HMFYAlzPw",
    "outputId": "3584b062-1d53-4b91-c780-f2cdf65565b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DetectionModule(\n",
       "  (multihead_attention): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_encoder.to(device)\n",
    "user_encoder.to(device)\n",
    "detection_module.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 113690,
     "status": "ok",
     "timestamp": 1746320087553,
     "user": {
      "displayName": "Zannatun Sristy",
      "userId": "17694043809177177827"
     },
     "user_tz": 300
    },
    "id": "dchkpc9q6M6y",
    "outputId": "c65a59b3-5188-4074-bf57-7a1425bbc09d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23705/23705 [02:14<00:00, 176.33it/s]\n",
      "100%|██████████| 18666/18666 [01:35<00:00, 195.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{23705}\n",
      "{18666}\n"
     ]
    }
   ],
   "source": [
    "news_vec_dict_train = build_news_vec_dict(Train_News_data, news_encoder, vocab_train, cat2idx_train, subcat2idx_train, device)\n",
    "news_vec_dict_val = build_news_vec_dict(Val_News_data, news_encoder, vocab_val, cat2idx_val, subcat2idx_val, device)\n",
    "\n",
    "news_vec_dict_train = move_news_vec_dict_to_device(news_vec_dict_train, device)\n",
    "news_vec_dict_val = move_news_vec_dict_to_device(news_vec_dict_val, device)\n",
    "\n",
    "print({len(news_vec_dict_train)})\n",
    "print({len(news_vec_dict_val)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDr8Tq9djsYY"
   },
   "source": [
    "#Code Component Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 258,
     "status": "ok",
     "timestamp": 1746320087808,
     "user": {
      "displayName": "Zannatun Sristy",
      "userId": "17694043809177177827"
     },
     "user_tz": 300
    },
    "id": "zOV9c16sjH4U",
    "outputId": "47f2c744-3874-4cc2-c3fa-df63d40b4ba8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2906, 0.9081, 0.4881, 0.0171, 0.5049, 0.0380, 0.9096, 0.5065, 0.7878,\n",
      "        0.5047, 0.2891, 0.5001], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "behavior_row = Train_Behaviors_data.iloc[1]\n",
    "clicked_news, candidate_news, candidate_labels = clicked_candidate_news_preparation(behavior_row, Train_News_data)\n",
    "\n",
    "cached_news_ids = set(news_vec_dict_train.keys())\n",
    "\n",
    "clicked_news = [nid for nid in clicked_news if nid in cached_news_ids]\n",
    "candidate_news = [nid for nid in candidate_news if nid in cached_news_ids]\n",
    "\n",
    "fc_click = torch.stack([news_vec_dict_train[nid][0] for nid in clicked_news])\n",
    "rt_click = torch.stack([news_vec_dict_train[nid][1] for nid in clicked_news])\n",
    "ra_click = torch.stack([news_vec_dict_train[nid][2] for nid in clicked_news])\n",
    "\n",
    "fc_cand = torch.stack([news_vec_dict_train[nid][0] for nid in candidate_news])\n",
    "rt_cand = torch.stack([news_vec_dict_train[nid][1] for nid in candidate_news])\n",
    "ra_cand = torch.stack([news_vec_dict_train[nid][2] for nid in candidate_news])\n",
    "\n",
    "detection_scores_clicked = detection_module(rt_click, ra_click).unsqueeze(0)\n",
    "detection_scores_cand = detection_module(rt_cand, ra_cand)\n",
    "\n",
    "user_vec = user_encoder(fc_click.unsqueeze(0), fc_cand[0:1], detection_score=detection_scores_clicked)\n",
    "click_scores = compute_click_scores(user_vec, fc_cand, detection_scores_cand)\n",
    "\n",
    "\n",
    "print(click_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AklbZifIsh3T"
   },
   "source": [
    "#Train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "otCufHBNRFia"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(behaviors_df, news_df, news_vec_dict_train, model_components, optimizer, vocab, cat2idx, subcat2idx):\n",
    "    user_encoder = model_components['user_encoder']\n",
    "    detection_module = model_components['detection_module']\n",
    "\n",
    "    user_encoder.train()\n",
    "    detection_module.train()\n",
    "\n",
    "    device = next(user_encoder.parameters()).device\n",
    "    total_loss, count = 0, 0\n",
    "\n",
    "    cached_news_ids = set(news_vec_dict_train.keys())\n",
    "\n",
    "    for _, behavior_row in tqdm(behaviors_df.iterrows()):\n",
    "        clicked_news, candidate_news, candidate_labels = clicked_candidate_news_preparation(behavior_row, news_df)\n",
    "        clicked_news = clicked_news[:50]\n",
    "        \n",
    "        if not candidate_labels or not clicked_news or not candidate_news:\n",
    "            continue\n",
    "\n",
    "        clicked_news = [nid for nid in clicked_news if nid in cached_news_ids]\n",
    "        candidate_news = [nid for nid in candidate_news if nid in cached_news_ids]\n",
    "        if not clicked_news or not candidate_news:\n",
    "            continue\n",
    "\n",
    "        fc_click = torch.stack([news_vec_dict_train[nid][0] for nid in clicked_news])\n",
    "        rt_click = torch.stack([news_vec_dict_train[nid][1] for nid in clicked_news])\n",
    "        ra_click = torch.stack([news_vec_dict_train[nid][2] for nid in clicked_news])\n",
    "\n",
    "        fc_cand = torch.stack([news_vec_dict_train[nid][0] for nid in candidate_news])\n",
    "        rt_cand = torch.stack([news_vec_dict_train[nid][1] for nid in candidate_news])\n",
    "        ra_cand = torch.stack([news_vec_dict_train[nid][2] for nid in candidate_news])\n",
    "\n",
    "        detection_scores_clicked = detection_module(rt_click, ra_click).unsqueeze(0)\n",
    "        detection_scores_cand = detection_module(rt_cand, ra_cand)\n",
    "\n",
    "        user_vec = user_encoder(fc_click.unsqueeze(0), fc_cand[0:1], detection_score=detection_scores_clicked)\n",
    "        click_scores = compute_click_scores(user_vec, fc_cand, detection_scores_cand)\n",
    "\n",
    "        labels = torch.tensor(candidate_labels, dtype=torch.float32, device=device)\n",
    "\n",
    "        pos_idx = (labels == 1).nonzero(as_tuple=True)[0]\n",
    "\n",
    "        if pos_idx.numel() == 1:\n",
    "            pos_idx = pos_idx.item()\n",
    "            pos_score = click_scores[pos_idx].unsqueeze(0)\n",
    "            neg_scores = torch.cat([click_scores[:pos_idx], click_scores[pos_idx + 1:]]).unsqueeze(0)\n",
    "\n",
    "            loss = ranking_softmax_loss(pos_score, neg_scores)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            count += 1\n",
    "\n",
    "    return total_loss / max(1, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "zqIPl78dYA7d"
   },
   "outputs": [],
   "source": [
    "def test_one_epoch(behaviors_df, news_df, news_vec_dict_val, voab, cat2idx, subcat2idx, model_components):\n",
    "    user_encoder = model_components['user_encoder']\n",
    "    detection_module = model_components['detection_module']\n",
    "\n",
    "    user_encoder.eval()\n",
    "    detection_module.eval()\n",
    "\n",
    "    device = next(user_encoder.parameters()).device\n",
    "    all_scores, all_labels = [], []\n",
    "\n",
    "    cached_news_ids = set(news_vec_dict_val.keys())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, behavior_row in tqdm(behaviors_df.iterrows()):\n",
    "            clicked_news, candidate_news, candidate_labels = clicked_candidate_news_preparation(behavior_row, news_df)\n",
    "            clicked_news = clicked_news[:50]\n",
    "            \n",
    "            if not candidate_labels or not clicked_news or not candidate_news:\n",
    "                continue\n",
    "\n",
    "            clicked_news = [nid for nid in clicked_news if nid in cached_news_ids]\n",
    "            candidate_news = [nid for nid in candidate_news if nid in cached_news_ids]\n",
    "            if not clicked_news or not candidate_news:\n",
    "                continue\n",
    "\n",
    "            fc_click = torch.stack([news_vec_dict_val[nid][0] for nid in clicked_news])\n",
    "            rt_click = torch.stack([news_vec_dict_val[nid][1] for nid in clicked_news])\n",
    "            ra_click = torch.stack([news_vec_dict_val[nid][2] for nid in clicked_news])\n",
    "\n",
    "            fc_cand = torch.stack([news_vec_dict_val[nid][0] for nid in candidate_news])\n",
    "            rt_cand = torch.stack([news_vec_dict_val[nid][1] for nid in candidate_news])\n",
    "            ra_cand = torch.stack([news_vec_dict_val[nid][2] for nid in candidate_news])\n",
    "\n",
    "            detection_scores_clicked = detection_module(rt_click, ra_click).unsqueeze(0)\n",
    "            detection_scores_cand = detection_module(rt_cand, ra_cand)\n",
    "\n",
    "            user_vec = user_encoder(fc_click.unsqueeze(0), fc_cand[0:1], detection_score=detection_scores_clicked)\n",
    "            click_scores = compute_click_scores(user_vec, fc_cand, detection_scores_cand)\n",
    "\n",
    "            all_scores.append(click_scores.cpu())\n",
    "            all_labels.append(torch.tensor(candidate_labels, dtype=torch.float32))\n",
    "\n",
    "    return eval_metrics(all_labels, all_scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "gN8u5XVO8t53"
   },
   "outputs": [],
   "source": [
    "model_components = {\n",
    "    'news_encoder': news_encoder,\n",
    "    'detection_module': detection_module,\n",
    "    'user_encoder': user_encoder\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "Ee9VPoEF8y7q"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    list(news_encoder.parameters()) +\n",
    "    list(user_encoder.parameters()) +\n",
    "    list(detection_module.parameters()),\n",
    "    lr=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xI6JZ6FUlsL2"
   },
   "source": [
    "#Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 156944,
     "status": "ok",
     "timestamp": 1746320244787,
     "user": {
      "displayName": "Zannatun Sristy",
      "userId": "17694043809177177827"
     },
     "user_tz": 300
    },
    "id": "P5ht49mL8qzu",
    "outputId": "480abf79-ae78-4b61-cdb1-229a16453c3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15667it [35:03,  7.45it/s]\n",
      "7369it [10:22, 11.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train Loss: 2.7018 | AUC: 0.4095, MRR: 0.2074, nDCG@5: 0.1809, nDCG@10: 0.2431\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15667it [36:26,  7.17it/s]\n",
      "7369it [10:33, 11.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 2.6911 | AUC: 0.4066, MRR: 0.2056, nDCG@5: 0.1789, nDCG@10: 0.2412\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15667it [34:52,  7.49it/s]\n",
      "7369it [10:39, 11.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 2.6847 | AUC: 0.4084, MRR: 0.2054, nDCG@5: 0.1801, nDCG@10: 0.2422\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15667it [36:59,  7.06it/s]\n",
      "7369it [10:45, 11.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 2.6753 | AUC: 0.4130, MRR: 0.2082, nDCG@5: 0.1819, nDCG@10: 0.2443\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15667it [35:53,  7.27it/s]\n",
      "7369it [10:29, 11.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 2.6699 | AUC: 0.4166, MRR: 0.2106, nDCG@5: 0.1850, nDCG@10: 0.2468\n",
      "-------------------------------------------Final Eval-------------------------------------------\n",
      "AUC: 41.66, MRR: 21.06, nDCG@5: 18.50, nDCG@10: 24.68\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "\n",
    "    train_loss = train_one_epoch(\n",
    "        Train_Behaviors_data,\n",
    "        Train_News_data,\n",
    "        news_vec_dict_train,\n",
    "        model_components,\n",
    "        optimizer,\n",
    "        vocab_train,\n",
    "        cat2idx_train,\n",
    "        subcat2idx_train\n",
    "    )\n",
    "\n",
    "    auc, mrr, ndcg5, ndcg10 = test_one_epoch(\n",
    "        Val_Behaviors_data,\n",
    "        Val_News_data,\n",
    "        news_vec_dict_val,\n",
    "        vocab_val,\n",
    "        cat2idx_val,\n",
    "        subcat2idx_val,\n",
    "        model_components\n",
    "    )\n",
    "\n",
    "    print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f} | \"\n",
    "          f\"AUC: {auc:.4f}, MRR: {mrr:.4f}, nDCG@5: {ndcg5:.4f}, nDCG@10: {ndcg10:.4f}\")\n",
    "\n",
    "print(\"-------------------------------------------Final Eval-------------------------------------------\")\n",
    "print(f\"AUC: {auc * 100:.2f}, MRR: {mrr * 100:.2f}, nDCG@5: {ndcg5 * 100:.2f}, nDCG@10: {ndcg10 * 100:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "EajA9i9P6TLc"
   },
   "outputs": [],
   "source": [
    "module_name = \"NQNR_PCNE\"\n",
    "\n",
    "df = pd.DataFrame([{\n",
    "    'Module': module_name,\n",
    "    'AUC': f\"{auc * 100:.2f}\",\n",
    "    'MRR': f\"{mrr * 100:.2f}\",\n",
    "    'nDCG@5': f\"{ndcg5 * 100:.2f}\",\n",
    "    'nDCG@10': f\"{ndcg10 * 100:.2f}\",\n",
    "}])\n",
    "\n",
    "filename = f\"{module_name}_results.csv\"\n",
    "\n",
    "df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Wzjz8O5cdCZ5"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python3.10",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
