{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtSKLcNPceDt"
   },
   "source": [
    "#Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "w6a-Y-DIX89P"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJKTSEDft1RQ"
   },
   "source": [
    "#Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PvRAaJ-P7wpC"
   },
   "outputs": [],
   "source": [
    "!unzip -q MINDsmall_train.zip -d train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 810,
     "status": "ok",
     "timestamp": 1746219441950,
     "user": {
      "displayName": "Zannatun Sristy",
      "userId": "17694043809177177827"
     },
     "user_tz": 300
    },
    "id": "wMDwdpanYxli",
    "outputId": "b7d488ec-2b8c-44ec-c84a-d381b158caa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51282, 8)\n"
     ]
    }
   ],
   "source": [
    "train_news_path = os.path.abspath('train/news.tsv')\n",
    "Train_News_data=pd.read_table(train_news_path,\n",
    "              header=None,\n",
    "              names=[\n",
    "                  'id', 'category', 'subcategory', 'title', 'abstract', 'url',\n",
    "                  'title_entities', 'abstract_entities'\n",
    "              ])\n",
    "\n",
    "print(Train_News_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1477,
     "status": "ok",
     "timestamp": 1746219443429,
     "user": {
      "displayName": "Zannatun Sristy",
      "userId": "17694043809177177827"
     },
     "user_tz": 300
    },
    "id": "ursaKx2xwEC_",
    "outputId": "8beeee5b-76af-458c-e0ae-0ef3e1717fab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156965, 5)\n"
     ]
    }
   ],
   "source": [
    "train_behaviors_path = os.path.abspath('train/behaviors.tsv')\n",
    "Train_Behaviors_data=pd.read_table(train_behaviors_path,\n",
    "              header=None,\n",
    "              names=[\n",
    "                  'impression_id', 'user_id', 'time', 'history', 'impressions'\n",
    "              ])\n",
    "print(Train_Behaviors_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "924ZSPqZt58-"
   },
   "source": [
    "#Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uHCiohg5t_LZ"
   },
   "outputs": [],
   "source": [
    "!unzip -q MINDsmall_dev.zip -d dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 926,
     "status": "ok",
     "timestamp": 1746219448372,
     "user": {
      "displayName": "Zannatun Sristy",
      "userId": "17694043809177177827"
     },
     "user_tz": 300
    },
    "id": "r0fa5MvOuI7R",
    "outputId": "6530c3db-c012-4a2a-8853-036a6dab9167"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42416, 8)\n"
     ]
    }
   ],
   "source": [
    "val_news_path = os.path.abspath('dev/news.tsv')\n",
    "Val_News_data=pd.read_table(val_news_path,\n",
    "              header=None,\n",
    "              names=[\n",
    "                  'id', 'category', 'subcategory', 'title', 'abstract', 'url',\n",
    "                  'title_entities', 'abstract_entities'\n",
    "              ])\n",
    "print(Val_News_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1052,
     "status": "ok",
     "timestamp": 1746219449422,
     "user": {
      "displayName": "Zannatun Sristy",
      "userId": "17694043809177177827"
     },
     "user_tz": 300
    },
    "id": "DV7EZ0kyv9jh",
    "outputId": "c02168c8-8d31-4ff5-b836-b5a85a74bf54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73152, 5)\n"
     ]
    }
   ],
   "source": [
    "val_behaviors_path = os.path.abspath('dev/behaviors.tsv')\n",
    "Val_Behaviors_data=pd.read_table(val_behaviors_path,\n",
    "              header=None,\n",
    "              names=[\n",
    "                  'impression_id', 'user_id', 'time', 'history', 'impressions'\n",
    "              ])\n",
    "print(Val_Behaviors_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "I-hTmBn0cOkg"
   },
   "outputs": [],
   "source": [
    "def subsample_train_val_same_users_split_by_source(\n",
    "    Train_News_data, Train_Behaviors_data,\n",
    "    Val_News_data, Val_Behaviors_data,\n",
    "    n_users=10000, train_ratio=2.1, seed=42\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    train_users = set(Train_Behaviors_data['user_id'].dropna())\n",
    "    val_users = set(Val_Behaviors_data['user_id'].dropna())\n",
    "    common_users = np.array(list(train_users & val_users))\n",
    "    common_user_ratio = len(train_users) / len(common_users)\n",
    "\n",
    "    common_user_count = min(int(n_users / common_user_ratio), len(common_users))\n",
    "    noncommon_user_count = n_users - common_user_count\n",
    "\n",
    "    sampled_common_users = rng.choice(common_users, size=common_user_count, replace=False)\n",
    "\n",
    "    train_unique_users = np.setdiff1d(list(train_users), common_users)\n",
    "    val_unique_users = np.setdiff1d(list(val_users), common_users)\n",
    "\n",
    "    sampled_train_unique = rng.choice(train_unique_users, size=noncommon_user_count, replace=False)\n",
    "    sampled_val_unique = rng.choice(val_unique_users, size=noncommon_user_count, replace=False)\n",
    "\n",
    "    user_train_logs = pd.concat([\n",
    "        Train_Behaviors_data[Train_Behaviors_data['user_id'].isin(sampled_common_users)],\n",
    "        Train_Behaviors_data[Train_Behaviors_data['user_id'].isin(sampled_train_unique)]\n",
    "    ])\n",
    "\n",
    "    user_val_logs = pd.concat([\n",
    "        Val_Behaviors_data[Val_Behaviors_data['user_id'].isin(sampled_common_users)],\n",
    "        Val_Behaviors_data[Val_Behaviors_data['user_id'].isin(sampled_val_unique)]\n",
    "    ])\n",
    "\n",
    "    target_val_size = int(len(user_train_logs) / train_ratio)\n",
    "    if target_val_size < len(user_val_logs):\n",
    "        user_val_logs = user_val_logs.sample(n=target_val_size, random_state=seed)\n",
    "\n",
    "    def get_referenced_news(news_df, behaviors_df):\n",
    "        news_ids = set()\n",
    "        for _, row in behaviors_df.iterrows():\n",
    "            history = str(row['history']) if not pd.isna(row['history']) else ''\n",
    "            news_ids.update(history.split())\n",
    "            impressions = str(row['impressions']) if not pd.isna(row['impressions']) else ''\n",
    "            news_ids.update(x.split('-')[0] for x in impressions.split())\n",
    "        return news_df[news_df['id'].astype(str).isin(news_ids)].copy()\n",
    "\n",
    "    train_news = get_referenced_news(Train_News_data, user_train_logs)\n",
    "    val_news = get_referenced_news(Val_News_data, user_val_logs)\n",
    "\n",
    "    return train_news, user_train_logs, val_news, user_val_logs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 87,
     "status": "ok",
     "timestamp": 1746219449540,
     "user": {
      "displayName": "Zannatun Sristy",
      "userId": "17694043809177177827"
     },
     "user_tz": 300
    },
    "id": "94L10-wxcTrv",
    "outputId": "6b1be916-cc60-4e19-b317-0cf813b01a1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train News Data Shape: (23854, 8)\n",
      "Train Behaviors Data Shape: (15661, 5)\n",
      "Valid News Data Shape: (18735, 8)\n",
      "Valid Behaviors Data Shape: (7358, 5)\n"
     ]
    }
   ],
   "source": [
    "# Subsample training data\n",
    "Train_News_data, Train_Behaviors_data, Val_News_data, Val_Behaviors_data = subsample_train_val_same_users_split_by_source(Train_News_data, Train_Behaviors_data, Val_News_data, Val_Behaviors_data, n_users=5000,  train_ratio=2.1)\n",
    "\n",
    "# Subsample validation data\n",
    "\n",
    "\n",
    "print(f\"Train News Data Shape: {Train_News_data.shape}\")\n",
    "print(f\"Train Behaviors Data Shape: {Train_Behaviors_data.shape}\")\n",
    "print(f\"Valid News Data Shape: {Val_News_data.shape}\")\n",
    "print(f\"Valid Behaviors Data Shape: {Val_Behaviors_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFc6hZfSvvmB"
   },
   "source": [
    "#Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 489166,
     "status": "ok",
     "timestamp": 1746219954632,
     "user": {
      "displayName": "Zannatun Sristy",
      "userId": "17694043809177177827"
     },
     "user_tz": 300
    },
    "id": "iKuZ7j2n4geD",
    "outputId": "79ef8be5-725f-456d-d94d-5fa370ccc547"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-05-02 20:57:45--  https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip [following]\n",
      "--2025-05-02 20:57:45--  https://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2176768927 (2.0G) [application/zip]\n",
      "Saving to: ‘glove.840B.300d.zip’\n",
      "\n",
      "glove.840B.300d.zip 100%[===================>]   2.03G  5.00MB/s    in 6m 50s  \n",
      "\n",
      "2025-05-02 21:04:36 (5.06 MB/s) - ‘glove.840B.300d.zip’ saved [2176768927/2176768927]\n",
      "\n",
      "Archive:  glove.840B.300d.zip\n",
      "  inflating: glove.840B.300d.txt     \n"
     ]
    }
   ],
   "source": [
    "!wget https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "!unzip glove.840B.300d.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Im58Wfw-xC4j"
   },
   "outputs": [],
   "source": [
    "def build_vocab(df, glove=None, min_freq=1):\n",
    "    counter = Counter()\n",
    "\n",
    "    for text in df['title'].fillna('').tolist() + df['abstract'].fillna('').tolist():\n",
    "        tokens = text.lower().split()\n",
    "        counter.update(tokens)\n",
    "\n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    oov_words = 0\n",
    "    in_glove_words = 0\n",
    "\n",
    "    for word, freq in counter.items():\n",
    "        if freq >= min_freq:\n",
    "            if glove is None or word in glove:\n",
    "                vocab[word] = len(vocab)\n",
    "                in_glove_words += 1\n",
    "            else:\n",
    "                vocab[word] = len(vocab)\n",
    "                oov_words += 1\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6sF76tH5pZ_o"
   },
   "outputs": [],
   "source": [
    "def build_category_indices(df):\n",
    "    cat2idx = {cat: idx for idx, cat in enumerate(df['category'].dropna().unique())}\n",
    "    subcat2idx = {subcat: idx for idx, subcat in enumerate(df['subcategory'].dropna().unique())}\n",
    "    return cat2idx, subcat2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-sUp8Ukqvz3n"
   },
   "outputs": [],
   "source": [
    "def tokenize(text, vocab, max_len=20):\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    tokens = text.lower().split()\n",
    "    token_ids = [vocab.get(w, vocab['<UNK>']) for w in tokens[:max_len]]\n",
    "    token_ids += [vocab['<PAD>']] * (max_len - len(token_ids))\n",
    "    return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "dqT9Zfx-I8sU"
   },
   "outputs": [],
   "source": [
    "def clicked_candidate_news_preparation(behaviors, news):\n",
    "    history_field = behaviors['history']\n",
    "    impressions_field = behaviors['impressions']\n",
    "\n",
    "    history_ids = history_field.split() if isinstance(history_field, str) and history_field.strip() else []\n",
    "    impression_pairs = impressions_field.split() if isinstance(impressions_field, str) and impressions_field.strip() else []\n",
    "\n",
    "    candidate_ids = [impr.split('-')[0] for impr in impression_pairs]\n",
    "    candidate_labels = [int(impr.split('-')[1]) for impr in impression_pairs]\n",
    "\n",
    "    clicked_news = news[news['id'].isin(history_ids)]\n",
    "    candidate_news = news[news['id'].isin(candidate_ids)]\n",
    "\n",
    "    return clicked_news, candidate_news, candidate_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HsrmmTNcqPY"
   },
   "source": [
    "#Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wzjz8O5cdCZ5"
   },
   "source": [
    "##Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "TbdXAh8vc_8O"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bi1HgGSEcyvu"
   },
   "source": [
    "##News Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ccezVPD1ZSdZ"
   },
   "outputs": [],
   "source": [
    "class NewsEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, label_size):\n",
    "        super(NewsEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.conv_groups = nn.ModuleList([\n",
    "            nn.Conv1d(embed_dim, embed_dim, kernel_size=3, padding=1) for _ in range(3)\n",
    "        ])\n",
    "\n",
    "        self.reduce_proj = nn.Linear(embed_dim * 3, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, embed_dim // 2, bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.title_att_W = nn.Linear(embed_dim, embed_dim)\n",
    "        self.title_att_b = nn.Parameter(torch.zeros(embed_dim))\n",
    "        self.title_att_v = nn.Linear(embed_dim, 1)\n",
    "\n",
    "        self.label_embed = nn.Embedding(label_size, embed_dim)\n",
    "        self.label_fc = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self.fusion_q = nn.Parameter(torch.randn(embed_dim))\n",
    "        self.fusion_Uv = nn.Linear(embed_dim, embed_dim)\n",
    "        self.fusion_uv = nn.Parameter(torch.zeros(embed_dim))\n",
    "\n",
    "    def text_encoder(self, text):\n",
    "        B = self.embedding(text).transpose(1, 2)\n",
    "        group_outputs = [conv(B) for conv in self.conv_groups]\n",
    "        B = F.relu(torch.cat(group_outputs, dim=1)).transpose(1, 2)\n",
    "        B = self.reduce_proj(B)\n",
    "        B, _ = self.lstm(B)\n",
    "        att = torch.tanh(self.title_att_W(B) + self.title_att_b)\n",
    "        alpha = F.softmax(self.title_att_v(att), dim=1)\n",
    "        return torch.sum(alpha * B, dim=1)\n",
    "\n",
    "    def label_encoder(self, x):\n",
    "        ex = self.label_embed(x)\n",
    "        return self.label_fc(ex)\n",
    "\n",
    "    def forward(self, title, abstract, category, subcategory):\n",
    "        rt = self.text_encoder(title)\n",
    "        ra = self.text_encoder(abstract)\n",
    "\n",
    "        rc = self.label_encoder(category)\n",
    "        rsc = self.label_encoder(subcategory)\n",
    "\n",
    "        at = torch.matmul(torch.tanh(self.fusion_Uv(rt) + self.fusion_uv), self.fusion_q)\n",
    "        aa = torch.matmul(torch.tanh(self.fusion_Uv(ra) + self.fusion_uv), self.fusion_q)\n",
    "        ac = torch.matmul(torch.tanh(self.fusion_Uv(rc) + self.fusion_uv), self.fusion_q)\n",
    "        asc = torch.matmul(torch.tanh(self.fusion_Uv(rsc) + self.fusion_uv), self.fusion_q)\n",
    "\n",
    "        weights = torch.stack([at, aa, ac, asc], dim=1)\n",
    "        alphas = F.softmax(weights, dim=1)\n",
    "        final_rep = (\n",
    "              alphas[:, 0].unsqueeze(1) * rt +\n",
    "              alphas[:, 1].unsqueeze(1) * ra +\n",
    "              alphas[:, 2].unsqueeze(1) * rc +\n",
    "              alphas[:, 3].unsqueeze(1) * rsc\n",
    "        )\n",
    "        return final_rep, rt, ra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlNzo0_M-NUp"
   },
   "source": [
    "##Detection Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "FsC5e0aKtlsl"
   },
   "outputs": [],
   "source": [
    "class DetectionModule(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, threshold=0.49, lambda_weight=0.8):\n",
    "        super(DetectionModule, self).__init__()\n",
    "        self.multihead_attention = nn.MultiheadAttention(embed_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.threshold = threshold\n",
    "        self.lambda_weight = lambda_weight\n",
    "\n",
    "\n",
    "    def forward(self, rt, ra):\n",
    "        rt = rt.unsqueeze(0)\n",
    "        ra = ra.unsqueeze(0)\n",
    "        tilda_rt,_ = self.multihead_attention(query=ra, key=rt, value=rt)\n",
    "\n",
    "        tilda_rt_norm = F.normalize(tilda_rt, p=2, dim=-1)\n",
    "        ra_norm = F.normalize(ra, p=2, dim=-1)\n",
    "\n",
    "        cos_sim = (tilda_rt_norm * ra_norm).sum(dim=-1)\n",
    "        pi = torch.sigmoid(cos_sim)\n",
    "\n",
    "        Si =  Si = torch.where(pi > self.threshold,\n",
    "                     torch.full_like(pi, self.lambda_weight),\n",
    "                     torch.ones_like(pi))\n",
    "        return Si.squeeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3YTOKDjLeZb"
   },
   "source": [
    "##User Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "51CaymkCLcuC"
   },
   "outputs": [],
   "source": [
    "class UserEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(UserEncoder, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.q_linear = nn.ModuleList([nn.Linear(embed_dim, embed_dim) for _ in range(num_heads)])\n",
    "        self.v_linear = nn.ModuleList([nn.Linear(embed_dim, embed_dim) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, history_vecs, candidate_vec, detection_score=None):\n",
    "        B, N, D = history_vecs.shape\n",
    "\n",
    "        head_outputs = []\n",
    "\n",
    "        for k in range(self.num_heads):\n",
    "            qk = self.q_linear[k](history_vecs)\n",
    "            scores = torch.matmul(qk, history_vecs.transpose(1, 2)) / (D ** 0.5)\n",
    "            beta = F.softmax(scores, dim=-1)\n",
    "\n",
    "            vk = self.v_linear[k](history_vecs)\n",
    "            head_output = torch.matmul(beta, vk)\n",
    "            head_outputs.append(head_output)\n",
    "\n",
    "        multihead_output = torch.cat(head_outputs, dim=-1)\n",
    "        multihead_output = multihead_output.view(B, N, self.num_heads, D).mean(dim=2)\n",
    "\n",
    "        candidate_vec = candidate_vec.unsqueeze(1)\n",
    "        dot_scores = (candidate_vec * multihead_output).sum(dim=2) / (D ** 0.5)\n",
    "\n",
    "        if detection_score is not None:\n",
    "            dot_scores = dot_scores * detection_score\n",
    "\n",
    "        alpha = F.softmax(dot_scores, dim=1)\n",
    "        user_vector = torch.bmm(alpha.unsqueeze(1), multihead_output).squeeze(1)\n",
    "        return user_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67Lkd6PMDG1q"
   },
   "source": [
    "#Score Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "-JXalL7PxVrB"
   },
   "outputs": [],
   "source": [
    "def compute_click_scores(user_vec, candidate_vecs, detection_scores=None):\n",
    "    B = candidate_vecs.shape[0]\n",
    "    user_vec_expand = user_vec.expand(B, -1)\n",
    "    dot_scores = (user_vec_expand * candidate_vecs).sum(dim=1)\n",
    "\n",
    "    if detection_scores is not None:\n",
    "        dot_scores = dot_scores * detection_scores\n",
    "\n",
    "    click_scores = torch.sigmoid(dot_scores)\n",
    "\n",
    "    return click_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8oteKukBdi7a"
   },
   "source": [
    "#Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "5CI4nnb_dc5u"
   },
   "outputs": [],
   "source": [
    "def ranking_softmax_loss(pos_scores, neg_scores):\n",
    "    pos_exp = torch.exp(-pos_scores)\n",
    "    neg_exp = torch.exp(-neg_scores).sum(dim=1)\n",
    "    denom = pos_exp + neg_exp\n",
    "    loss = -torch.log(pos_exp / denom)\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Us1JcN6uI0_7"
   },
   "source": [
    "#Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "LfEA_0oNI2U2"
   },
   "outputs": [],
   "source": [
    "def encode(row, max_len, vocab, cat2idx, subcat2idx):\n",
    "        title = tokenize(row['title'], vocab, max_len)\n",
    "        abstract = tokenize(row['abstract'], vocab, max_len)\n",
    "        category = cat2idx.get(row['category'], 0)\n",
    "        subcategory = subcat2idx.get(row['subcategory'], 0)\n",
    "        return title, abstract, category, subcategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "xt0Jik8ZiY7N"
   },
   "outputs": [],
   "source": [
    "def encode_news_batch(news_rows, vocab, cat2idx, subcat2idx, max_len=20):\n",
    "\n",
    "    encoded = [encode(row, max_len, vocab, cat2idx, subcat2idx) for _, row in news_rows.iterrows()]\n",
    "    titles = torch.tensor([e[0] for e in encoded])\n",
    "    abstracts = torch.tensor([e[1] for e in encoded])\n",
    "    categories = torch.tensor([e[2] for e in encoded])\n",
    "    subcategories = torch.tensor([e[3] for e in encoded])\n",
    "\n",
    "    return titles, abstracts, categories, subcategories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "1D_pGDwaYQGm"
   },
   "outputs": [],
   "source": [
    "def move_to_device(device, *tensors):\n",
    "    return [t.to(device) for t in tensors]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xofUHpwHq9T"
   },
   "source": [
    "#Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "enbeVBeeIK8V"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "TSeicvuMWr4z"
   },
   "outputs": [],
   "source": [
    "def compute_mrr(y_true_sorted):\n",
    "    y_true_sorted = np.asarray(y_true_sorted)\n",
    "    relevant_indices = np.where(y_true_sorted == 1)[0]\n",
    "\n",
    "    if relevant_indices.size == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return 1.0 / (relevant_indices[0] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "oDwRUPfCHw4c"
   },
   "outputs": [],
   "source": [
    "def ndcg(y_true_sorted, k):\n",
    "    y_true_sorted = np.asarray(y_true_sorted)\n",
    "    k = min(k, len(y_true_sorted))\n",
    "    if k == 0:\n",
    "        return 0.0\n",
    "\n",
    "    gains = 2 ** y_true_sorted[:k] - 1\n",
    "    discounts = np.log2(np.arange(2, k + 2))\n",
    "    dcg = np.sum(gains / discounts)\n",
    "\n",
    "    ideal_sorted = np.sort(y_true_sorted)[::-1]\n",
    "    ideal_gains = 2 ** ideal_sorted[:k] - 1\n",
    "    ideal_dcg = np.sum(ideal_gains / discounts)\n",
    "\n",
    "    return dcg / ideal_dcg if ideal_dcg > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "_9l9CWpsITtZ"
   },
   "outputs": [],
   "source": [
    "def eval_metrics(y_true, y_score):\n",
    "    aucs, mrrs, ndcg5s, ndcg10s = [], [], [], []\n",
    "\n",
    "    for labels, scores in zip(y_true, y_score):\n",
    "        sorted_idx = torch.argsort(scores, descending=True)\n",
    "        sorted_labels = labels[sorted_idx]\n",
    "\n",
    "        auc = roc_auc_score(labels.numpy(), scores.numpy()) if len(set(labels.numpy())) > 1 else 0.0\n",
    "        mrr = compute_mrr(sorted_labels)\n",
    "        ndcg5 = ndcg(sorted_labels, 5)\n",
    "        ndcg10 = ndcg(sorted_labels, 10)\n",
    "\n",
    "        aucs.append(auc)\n",
    "        mrrs.append(mrr)\n",
    "        ndcg5s.append(ndcg5)\n",
    "        ndcg10s.append(ndcg10)\n",
    "\n",
    "    auc = sum(aucs) / len(aucs)\n",
    "    mrr = sum(mrrs) / len(mrrs)\n",
    "    ndcg5 = sum(ndcg5s) / len(ndcg5s)\n",
    "    ndcg10 = sum(ndcg10s) / len(ndcg10s)\n",
    "\n",
    "    return auc, mrr, ndcg5, ndcg10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93jlgWByllIO"
   },
   "source": [
    "#Model Object Initiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "aMOq5hs_TIhS"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "1xws_dBk9FhO"
   },
   "outputs": [],
   "source": [
    "def load_glove_embeddings(glove_path, vocab=None, embed_dim=300):\n",
    "    glove = {}\n",
    "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f):\n",
    "            values = line.strip().split()\n",
    "            if len(values) != embed_dim + 1:\n",
    "                continue\n",
    "            word = values[0]\n",
    "\n",
    "            if vocab is not None and word not in vocab:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                vector = torch.tensor([float(v) for v in values[1:]], dtype=torch.float32)\n",
    "                glove[word] = vector\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    return glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 294380,
     "status": "ok",
     "timestamp": 1746221451150,
     "user": {
      "displayName": "Zannatun Sristy",
      "userId": "17694043809177177827"
     },
     "user_tz": 300
    },
    "id": "xoPjdM-Cq98d",
    "outputId": "bba75c4a-376f-4e02-a223-1a20957970c9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [01:40, 21806.59it/s]\n"
     ]
    }
   ],
   "source": [
    "glove = load_glove_embeddings('glove.840B.300d.txt')\n",
    "\n",
    "vocab = build_vocab(Train_News_data, glove)\n",
    "cat2idx, subcat2idx = build_category_indices(Train_News_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "UH98q1lFjut4"
   },
   "outputs": [],
   "source": [
    "vocab_test = build_vocab(Val_News_data, glove)\n",
    "cat2idx_test, subcat2idx_test = build_category_indices(Val_News_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "7h2dgrw1rYhy"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embed_dim = 300\n",
    "label_size = max(len(cat2idx), len(subcat2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "TfVswNnelhxb"
   },
   "outputs": [],
   "source": [
    "news_encoder = NewsEncoder(vocab_size, embed_dim, label_size)\n",
    "user_encoder = UserEncoder(embed_dim=300, num_heads=6)\n",
    "detection_module = DetectionModule(embed_dim=300, num_heads=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 178,
     "status": "ok",
     "timestamp": 1746221556796,
     "user": {
      "displayName": "Zannatun Sristy",
      "userId": "17694043809177177827"
     },
     "user_tz": 300
    },
    "id": "sc9HMFYAlzPw",
    "outputId": "5361b896-369b-47fc-aa7a-503e6932b93f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DetectionModule(\n",
       "  (multihead_attention): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "news_encoder.to(device)\n",
    "user_encoder.to(device)\n",
    "detection_module.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDr8Tq9djsYY"
   },
   "source": [
    "#Code Component Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1037,
     "status": "ok",
     "timestamp": 1746221562764,
     "user": {
      "displayName": "Zannatun Sristy",
      "userId": "17694043809177177827"
     },
     "user_tz": 300
    },
    "id": "zOV9c16sjH4U",
    "outputId": "b4f00d00-1feb-496d-c4e0-5feb917eb58f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5023, 0.1948, 0.1577, 0.3979, 0.1994], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "behavior_row = Train_Behaviors_data.iloc[0]\n",
    "clicked_news, candidate_news, candidate_labels = clicked_candidate_news_preparation(behavior_row, Train_News_data)\n",
    "\n",
    "\n",
    "titles_clicked, abstracts_clicked, cats_clicked, subcats_clicked = encode_news_batch(clicked_news, vocab, cat2idx, subcat2idx)\n",
    "titles_cand, abstracts_cand, cats_cand, subcats_cand = encode_news_batch(candidate_news, vocab, cat2idx, subcat2idx)\n",
    "\n",
    "\n",
    "titles_clicked, abstracts_clicked, cats_clicked, subcats_clicked = move_to_device(\n",
    "    device, titles_clicked, abstracts_clicked, cats_clicked, subcats_clicked\n",
    ")\n",
    "\n",
    "titles_cand, abstracts_cand, cats_cand, subcats_cand = move_to_device(\n",
    "    device, titles_cand, abstracts_cand, cats_cand, subcats_cand\n",
    ")\n",
    "\n",
    "\n",
    "final_clicked_vecs, rts_clicked, ras_clicked = news_encoder(titles_clicked, abstracts_clicked, cats_clicked, subcats_clicked)\n",
    "final_cand_vecs, rt_cand, ra_cand = news_encoder(titles_cand, abstracts_cand, cats_cand, subcats_cand)\n",
    "\n",
    "\n",
    "# detection_scores_clicked = detection_module(rts_clicked, ras_clicked).unsqueeze(0)\n",
    "# detection_scores_cand = detection_module(rt_cand, ra_cand)\n",
    "\n",
    "\n",
    "history_vecs = final_clicked_vecs.unsqueeze(0)\n",
    "user_vec = user_encoder(history_vecs, final_cand_vecs[0:1])\n",
    "\n",
    "\n",
    "click_scores = compute_click_scores(user_vec, final_cand_vecs)\n",
    "\n",
    "print(click_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AklbZifIsh3T"
   },
   "source": [
    "#Train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "otCufHBNRFia"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(behaviors_df, news_df, model_components, optimizer, vocab, cat2idx, subcat2idx):\n",
    "    news_encoder = model_components['news_encoder']\n",
    "    user_encoder = model_components['user_encoder']\n",
    "    detection_module = model_components['detection_module']\n",
    "\n",
    "    news_encoder.train()\n",
    "    user_encoder.train()\n",
    "    detection_module.train()\n",
    "\n",
    "    device = next(news_encoder.parameters()).device\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    for _, behavior_row in tqdm(behaviors_df.iterrows()):\n",
    "        clicked_news, candidate_news, candidate_labels = clicked_candidate_news_preparation(behavior_row, news_df)\n",
    "        clicked_news = clicked_news[:50]\n",
    "        \n",
    "        if len(candidate_labels) == 0 or clicked_news.empty or candidate_news.empty:\n",
    "            continue\n",
    "\n",
    "        titles_clicked, abstracts_clicked, cats_clicked, subcats_clicked = encode_news_batch(clicked_news, vocab, cat2idx, subcat2idx)\n",
    "        titles_cand, abstracts_cand, cats_cand, subcats_cand = encode_news_batch(candidate_news, vocab, cat2idx, subcat2idx)\n",
    "\n",
    "\n",
    "        titles_clicked, abstracts_clicked, cats_clicked, subcats_clicked = move_to_device(\n",
    "            device, titles_clicked, abstracts_clicked, cats_clicked, subcats_clicked\n",
    "        )\n",
    "        titles_cand, abstracts_cand, cats_cand, subcats_cand = move_to_device(\n",
    "            device, titles_cand, abstracts_cand, cats_cand, subcats_cand\n",
    "        )\n",
    "\n",
    "\n",
    "        final_clicked_vecs, rts_clicked, ras_clicked = news_encoder(titles_clicked, abstracts_clicked, cats_clicked, subcats_clicked)\n",
    "        final_cand_vecs, rt_cand, ra_cand = news_encoder(titles_cand, abstracts_cand, cats_cand, subcats_cand)\n",
    "\n",
    "\n",
    "        # detection_scores_clicked = detection_module(rts_clicked, ras_clicked).unsqueeze(0)\n",
    "        # detection_scores_cand = detection_module(rt_cand, ra_cand)\n",
    "\n",
    "\n",
    "        history_vecs = final_clicked_vecs.unsqueeze(0)\n",
    "        user_vec = user_encoder(history_vecs, final_cand_vecs[0:1])\n",
    "\n",
    "\n",
    "        click_scores = compute_click_scores(user_vec, final_cand_vecs)\n",
    "\n",
    "\n",
    "        labels = torch.tensor(candidate_labels, dtype=torch.float32).to(device)\n",
    "\n",
    "        pos_idx = (labels == 1).nonzero(as_tuple=True)[0]\n",
    "\n",
    "        if pos_idx.numel() == 1:\n",
    "            pos_idx = pos_idx.item()\n",
    "            pos_score = click_scores[pos_idx].unsqueeze(0)\n",
    "            neg_scores = torch.cat([click_scores[:pos_idx], click_scores[pos_idx + 1:]]).unsqueeze(0)\n",
    "\n",
    "            loss = ranking_softmax_loss(pos_score, neg_scores)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            count += 1\n",
    "\n",
    "    return total_loss / max(1, count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "zqIPl78dYA7d"
   },
   "outputs": [],
   "source": [
    "def test_one_epoch(behaviors_df, news_df, vocab, cat2idx, subcat2idx, model_components):\n",
    "    news_encoder = model_components['news_encoder']\n",
    "    user_encoder = model_components['user_encoder']\n",
    "    detection_module = model_components['detection_module']\n",
    "\n",
    "    news_encoder.eval()\n",
    "    user_encoder.eval()\n",
    "    detection_module.eval()\n",
    "\n",
    "    device = next(news_encoder.parameters()).device\n",
    "    all_scores, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, behavior_row in tqdm(behaviors_df.iterrows()):\n",
    "            clicked_news, candidate_news, candidate_labels = clicked_candidate_news_preparation(behavior_row, news_df)\n",
    "            clicked_news = clicked_news[:50]\n",
    "            \n",
    "            if len(candidate_labels) == 0 or clicked_news.empty or candidate_news.empty:\n",
    "                continue\n",
    "\n",
    "            titles_clicked, abstracts_clicked, cats_clicked, subcats_clicked = encode_news_batch(clicked_news, vocab, cat2idx, subcat2idx)\n",
    "            titles_cand, abstracts_cand, cats_cand, subcats_cand = encode_news_batch(candidate_news, vocab, cat2idx, subcat2idx)\n",
    "\n",
    "\n",
    "            titles_clicked, abstracts_clicked, cats_clicked, subcats_clicked = move_to_device(\n",
    "                device, titles_clicked, abstracts_clicked, cats_clicked, subcats_clicked\n",
    "            )\n",
    "\n",
    "            titles_cand, abstracts_cand, cats_cand, subcats_cand = move_to_device(\n",
    "                device, titles_cand, abstracts_cand, cats_cand, subcats_cand\n",
    "            )\n",
    "\n",
    "\n",
    "            final_clicked_vecs, rts_clicked, ras_clicked = news_encoder(titles_clicked, abstracts_clicked, cats_clicked, subcats_clicked)\n",
    "            final_cand_vecs, rt_cand, ra_cand = news_encoder(titles_cand, abstracts_cand, cats_cand, subcats_cand)\n",
    "\n",
    "\n",
    "            # detection_scores_clicked = detection_module(rts_clicked, ras_clicked).unsqueeze(0)\n",
    "            # detection_scores_cand = detection_module(rt_cand, ra_cand)\n",
    "\n",
    "\n",
    "            history_vecs = final_clicked_vecs.unsqueeze(0)\n",
    "            user_vec = user_encoder(history_vecs, final_cand_vecs[0:1])\n",
    "\n",
    "\n",
    "            click_scores = compute_click_scores(user_vec, final_cand_vecs)\n",
    "\n",
    "\n",
    "            all_scores.append(click_scores.detach().cpu())\n",
    "            all_labels.append(torch.tensor(candidate_labels, dtype=torch.float32))\n",
    "\n",
    "    return eval_metrics(all_labels, all_scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fk54vasF6-_L"
   },
   "source": [
    "#Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1574138,
     "status": "ok",
     "timestamp": 1746223240418,
     "user": {
      "displayName": "Zannatun Sristy",
      "userId": "17694043809177177827"
     },
     "user_tz": 300
    },
    "id": "cHdO2dL8Ycpx",
    "outputId": "825c1471-283b-4f4b-8dca-045c36290be2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15661it [43:01,  6.07it/s]\n",
      "7358it [07:20, 16.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train Loss: 2.8333 | AUC: 0.4954, MRR: 0.2422, nDCG@5: 0.2192, nDCG@10: 0.2824\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15661it [42:08,  6.19it/s]\n",
      "7358it [07:14, 16.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 2.7814 | AUC: 0.4956, MRR: 0.2422, nDCG@5: 0.2191, nDCG@10: 0.2824\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15661it [43:29,  6.00it/s]\n",
      "7358it [07:22, 16.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 2.7814 | AUC: 0.4939, MRR: 0.2421, nDCG@5: 0.2200, nDCG@10: 0.2816\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15661it [43:00,  6.07it/s]\n",
      "7358it [06:45, 18.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 2.7814 | AUC: 0.4969, MRR: 0.2435, nDCG@5: 0.2216, nDCG@10: 0.2828\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15661it [43:43,  5.97it/s]\n",
      "7358it [06:22, 19.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 2.7814 | AUC: 0.4964, MRR: 0.2433, nDCG@5: 0.2215, nDCG@10: 0.2831\n",
      "-------------------------------------------Final Eval-------------------------------------------\n",
      "AUC: 49.64, MRR: 24.33, nDCG@5: 22.15, nDCG@10: 28.31\n"
     ]
    }
   ],
   "source": [
    "model_components = {\n",
    "    'news_encoder': news_encoder,\n",
    "    'detection_module': detection_module,\n",
    "    'user_encoder': user_encoder\n",
    "}\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(news_encoder.parameters()) +\n",
    "    list(user_encoder.parameters()) +\n",
    "    list(detection_module.parameters()), lr=1e-4\n",
    ")\n",
    "\n",
    "for epoch in range(0, 5):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    train_loss = train_one_epoch(Train_Behaviors_data, Train_News_data, model_components, optimizer, vocab, cat2idx, subcat2idx)\n",
    "    auc, mrr, ndcg5, ndcg10 = test_one_epoch(Val_Behaviors_data, Val_News_data, vocab_test, cat2idx_test, subcat2idx_test, model_components)\n",
    "    print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f} | \"\n",
    "          f\"AUC: {auc:.4f}, MRR: {mrr:.4f}, nDCG@5: {ndcg5:.4f}, nDCG@10: {ndcg10:.4f}\")\n",
    "\n",
    "print(\"-------------------------------------------Final Eval-------------------------------------------\")\n",
    "print(f\"AUC: {auc * 100:.2f}, MRR: {mrr * 100:.2f}, nDCG@5: {ndcg5 * 100:.2f}, nDCG@10: {ndcg10 * 100:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_name = \"NQNR_without_DM\"\n",
    "\n",
    "df = pd.DataFrame([{\n",
    "    'Module': module_name,\n",
    "    'AUC': f\"{auc * 100:.2f}\",\n",
    "    'MRR': f\"{mrr * 100:.2f}\",\n",
    "    'nDCG@5': f\"{ndcg5 * 100:.2f}\",\n",
    "    'nDCG@10': f\"{ndcg10 * 100:.2f}\",\n",
    "}])\n",
    "\n",
    "filename = f\"{module_name}_results.csv\"\n",
    "\n",
    "df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python3.10",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
