{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtSKLcNPceDt"
   },
   "source": [
    "#Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "w6a-Y-DIX89P"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm as tqdm\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJKTSEDft1RQ"
   },
   "source": [
    "#Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PvRAaJ-P7wpC"
   },
   "outputs": [],
   "source": [
    "!unzip -q MINDsmall_train.zip -d train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 997,
     "status": "ok",
     "timestamp": 1746447041113,
     "user": {
      "displayName": "Zannatun Sristy",
      "userId": "17694043809177177827"
     },
     "user_tz": 300
    },
    "id": "wMDwdpanYxli",
    "outputId": "8026da71-2dff-4bf0-c4e6-b9cb7ee00eea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51282, 8)\n"
     ]
    }
   ],
   "source": [
    "train_news_path = os.path.abspath('train/news.tsv')\n",
    "Train_News_data=pd.read_table(train_news_path,\n",
    "              header=None,\n",
    "              names=[\n",
    "                  'id', 'category', 'subcategory', 'title', 'abstract', 'url',\n",
    "                  'title_entities', 'abstract_entities'\n",
    "              ])\n",
    "\n",
    "print(Train_News_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1601,
     "status": "ok",
     "timestamp": 1746447043676,
     "user": {
      "displayName": "Zannatun Sristy",
      "userId": "17694043809177177827"
     },
     "user_tz": 300
    },
    "id": "ursaKx2xwEC_",
    "outputId": "c72f8801-7457-47b6-dfc7-58fadfdeac14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156965, 5)\n"
     ]
    }
   ],
   "source": [
    "train_behaviors_path = os.path.abspath('train/behaviors.tsv')\n",
    "Train_Behaviors_data=pd.read_table(train_behaviors_path,\n",
    "              header=None,\n",
    "              names=[\n",
    "                  'impression_id', 'user_id', 'time', 'history', 'impressions'\n",
    "              ])\n",
    "print(Train_Behaviors_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "924ZSPqZt58-"
   },
   "source": [
    "#Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uHCiohg5t_LZ"
   },
   "outputs": [],
   "source": [
    "!unzip -q MINDsmall_dev.zip -d dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 782,
     "status": "ok",
     "timestamp": 1746447045350,
     "user": {
      "displayName": "Zannatun Sristy",
      "userId": "17694043809177177827"
     },
     "user_tz": 300
    },
    "id": "r0fa5MvOuI7R",
    "outputId": "d3cd24a3-e826-4519-867f-bf4096998695"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42416, 8)\n"
     ]
    }
   ],
   "source": [
    "val_news_path = os.path.abspath('dev/news.tsv')\n",
    "Val_News_data=pd.read_table(val_news_path,\n",
    "              header=None,\n",
    "              names=[\n",
    "                  'id', 'category', 'subcategory', 'title', 'abstract', 'url',\n",
    "                  'title_entities', 'abstract_entities'\n",
    "              ])\n",
    "print(Val_News_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 872,
     "status": "ok",
     "timestamp": 1746447047269,
     "user": {
      "displayName": "Zannatun Sristy",
      "userId": "17694043809177177827"
     },
     "user_tz": 300
    },
    "id": "DV7EZ0kyv9jh",
    "outputId": "fee2203e-3ace-4a76-eedb-375a4826bf85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73152, 5)\n"
     ]
    }
   ],
   "source": [
    "val_behaviors_path = os.path.abspath('dev/behaviors.tsv')\n",
    "Val_Behaviors_data=pd.read_table(val_behaviors_path,\n",
    "              header=None,\n",
    "              names=[\n",
    "                  'impression_id', 'user_id', 'time', 'history', 'impressions'\n",
    "              ])\n",
    "print(Val_Behaviors_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pia48VKq7OUf"
   },
   "source": [
    "#Subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "I-hTmBn0cOkg"
   },
   "outputs": [],
   "source": [
    "def subsample_train_val_same_users_split_by_source(\n",
    "    Train_News_data, Train_Behaviors_data,\n",
    "    Val_News_data, Val_Behaviors_data,\n",
    "    n_users=10000, train_ratio=2.1, seed=42\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    train_users = set(Train_Behaviors_data['user_id'].dropna())\n",
    "    val_users = set(Val_Behaviors_data['user_id'].dropna())\n",
    "    common_users = np.array(list(train_users & val_users))\n",
    "    common_user_ratio = len(train_users) / len(common_users)\n",
    "\n",
    "    common_user_count = min(int(n_users / common_user_ratio), len(common_users))\n",
    "    noncommon_user_count = n_users - common_user_count\n",
    "\n",
    "    sampled_common_users = rng.choice(common_users, size=common_user_count, replace=False)\n",
    "\n",
    "    train_unique_users = np.setdiff1d(list(train_users), common_users)\n",
    "    val_unique_users = np.setdiff1d(list(val_users), common_users)\n",
    "\n",
    "    sampled_train_unique = rng.choice(train_unique_users, size=noncommon_user_count, replace=False)\n",
    "    sampled_val_unique = rng.choice(val_unique_users, size=noncommon_user_count, replace=False)\n",
    "\n",
    "    user_train_logs = pd.concat([\n",
    "        Train_Behaviors_data[Train_Behaviors_data['user_id'].isin(sampled_common_users)],\n",
    "        Train_Behaviors_data[Train_Behaviors_data['user_id'].isin(sampled_train_unique)]\n",
    "    ])\n",
    "\n",
    "    user_val_logs = pd.concat([\n",
    "        Val_Behaviors_data[Val_Behaviors_data['user_id'].isin(sampled_common_users)],\n",
    "        Val_Behaviors_data[Val_Behaviors_data['user_id'].isin(sampled_val_unique)]\n",
    "    ])\n",
    "\n",
    "    target_val_size = int(len(user_train_logs) / train_ratio)\n",
    "    if target_val_size < len(user_val_logs):\n",
    "        user_val_logs = user_val_logs.sample(n=target_val_size, random_state=seed)\n",
    "\n",
    "    def get_referenced_news(news_df, behaviors_df):\n",
    "        news_ids = set()\n",
    "        for _, row in behaviors_df.iterrows():\n",
    "            history = str(row['history']) if not pd.isna(row['history']) else ''\n",
    "            news_ids.update(history.split())\n",
    "            impressions = str(row['impressions']) if not pd.isna(row['impressions']) else ''\n",
    "            news_ids.update(x.split('-')[0] for x in impressions.split())\n",
    "        return news_df[news_df['id'].astype(str).isin(news_ids)].copy()\n",
    "\n",
    "    train_news = get_referenced_news(Train_News_data, user_train_logs)\n",
    "    val_news = get_referenced_news(Val_News_data, user_val_logs)\n",
    "\n",
    "    return train_news, user_train_logs, val_news, user_val_logs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 464,
     "status": "ok",
     "timestamp": 1746447059923,
     "user": {
      "displayName": "Zannatun Sristy",
      "userId": "17694043809177177827"
     },
     "user_tz": 300
    },
    "id": "94L10-wxcTrv",
    "outputId": "55a97f9f-904f-4a5c-f1f1-b6130f1e616a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train News Data Shape: (23560, 8)\n",
      "Train Behaviors Data Shape: (15698, 5)\n",
      "Valid News Data Shape: (18612, 8)\n",
      "Valid Behaviors Data Shape: (7378, 5)\n"
     ]
    }
   ],
   "source": [
    "# Subsample training data\n",
    "Train_News_data, Train_Behaviors_data, Val_News_data, Val_Behaviors_data = subsample_train_val_same_users_split_by_source(Train_News_data, Train_Behaviors_data, Val_News_data, Val_Behaviors_data, n_users=5000,  train_ratio=2.1)\n",
    "\n",
    "# Subsample validation data\n",
    "\n",
    "\n",
    "print(f\"Train News Data Shape: {Train_News_data.shape}\")\n",
    "print(f\"Train Behaviors Data Shape: {Train_Behaviors_data.shape}\")\n",
    "print(f\"Valid News Data Shape: {Val_News_data.shape}\")\n",
    "print(f\"Valid Behaviors Data Shape: {Val_Behaviors_data.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFc6hZfSvvmB"
   },
   "source": [
    "#Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 497408,
     "status": "ok",
     "timestamp": 1746444239007,
     "user": {
      "displayName": "Zannatun Sristy",
      "userId": "17694043809177177827"
     },
     "user_tz": 300
    },
    "id": "iKuZ7j2n4geD",
    "outputId": "75bc2c7d-a6aa-44e7-fbe2-ed15886a4394"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-05-05 11:15:43--  https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip [following]\n",
      "--2025-05-05 11:15:43--  https://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2176768927 (2.0G) [application/zip]\n",
      "Saving to: ‘glove.840B.300d.zip’\n",
      "\n",
      "glove.840B.300d.zip 100%[===================>]   2.03G  5.01MB/s    in 6m 49s  \n",
      "\n",
      "2025-05-05 11:22:33 (5.07 MB/s) - ‘glove.840B.300d.zip’ saved [2176768927/2176768927]\n",
      "\n",
      "Archive:  glove.840B.300d.zip\n",
      "  inflating: glove.840B.300d.txt     \n"
     ]
    }
   ],
   "source": [
    "!wget https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "!unzip glove.840B.300d.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1xws_dBk9FhO"
   },
   "outputs": [],
   "source": [
    "def load_glove_embeddings(glove_path, vocab=None, embed_dim=300):\n",
    "    glove = {}\n",
    "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f):\n",
    "            values = line.strip().split()\n",
    "            if len(values) != embed_dim + 1:\n",
    "                continue\n",
    "            word = values[0]\n",
    "\n",
    "            if vocab is not None and word not in vocab:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                vector = torch.tensor([float(v) for v in values[1:]], dtype=torch.float32)\n",
    "                glove[word] = vector\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    return glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Im58Wfw-xC4j"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def extract_entities(entity_str):\n",
    "    try:\n",
    "        entities = json.loads(entity_str)\n",
    "        return \" \".join(e.get(\"Entity\", \"\") for e in entities)\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def build_vocab(df, glove=None, min_freq=1):\n",
    "    counter = Counter()\n",
    "\n",
    "    titles = df['title'].fillna('').tolist()\n",
    "    abstracts = df['abstract'].fillna('').tolist()\n",
    "\n",
    "    title_entities = df['title_entities'].fillna('').apply(extract_entities).tolist()\n",
    "    abstract_entities = df['abstract_entities'].fillna('').apply(extract_entities).tolist()\n",
    "\n",
    "    texts = titles + abstracts + title_entities + abstract_entities\n",
    "\n",
    "    for text in texts:\n",
    "        tokens = text.lower().split()\n",
    "        counter.update(tokens)\n",
    "\n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    oov_words = 0\n",
    "    in_glove_words = 0\n",
    "\n",
    "    for word, freq in counter.items():\n",
    "        if freq >= min_freq:\n",
    "            if glove is None or word in glove:\n",
    "                vocab[word] = len(vocab)\n",
    "                in_glove_words += 1\n",
    "            else:\n",
    "                vocab[word] = len(vocab)\n",
    "                oov_words += 1\n",
    "\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-sUp8Ukqvz3n"
   },
   "outputs": [],
   "source": [
    "def tokenize(text, vocab, max_len=20):\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    tokens = text.lower().split()\n",
    "    token_ids = [vocab.get(w, vocab['<UNK>']) for w in tokens[:max_len]]\n",
    "    token_ids += [vocab['<PAD>']] * (max_len - len(token_ids))\n",
    "    return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "l_2zTqEKv4xt"
   },
   "outputs": [],
   "source": [
    "def create_embedding_matrix(vocab, glove, embed_dim=300):\n",
    "    matrix = torch.randn(len(vocab), embed_dim) * 0.01\n",
    "    for word, idx in vocab.items():\n",
    "        if word in glove:\n",
    "            matrix[idx] = glove[word]\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "rBeIM-ybtqCC"
   },
   "outputs": [],
   "source": [
    "def build_news_dict(df, vocab, max_title_len=20, max_abstract_len=50):\n",
    "    news_dict = {}\n",
    "    newsid2cat = {}\n",
    "    category_news_dict = defaultdict(list)\n",
    "\n",
    "    unique_categories = df['category'].dropna().unique()\n",
    "    cat2idx = {cat: idx for idx, cat in enumerate(unique_categories)}\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        nid = row['id']\n",
    "\n",
    "        title = row.get('title', '')\n",
    "        abstract = row.get('abstract', '')\n",
    "        title_entities = extract_entities(row.get('title_entities', ''))\n",
    "        abstract_entities = extract_entities(row.get('abstract_entities', ''))\n",
    "\n",
    "        title_indices = tokenize(title, vocab, max_len=max_title_len)\n",
    "        abstract_indices = tokenize(abstract, vocab, max_len=max_abstract_len)\n",
    "        title_entity_indices = tokenize(title_entities, vocab, max_len=max_title_len)\n",
    "        abstract_entity_indices = tokenize(abstract_entities, vocab, max_len=max_abstract_len)\n",
    "\n",
    "        news_dict[nid] = {\n",
    "            'title_indices': title_indices,\n",
    "            'abstract_indices': abstract_indices,\n",
    "            'title_entity_indices': title_entity_indices,\n",
    "            'abstract_entity_indices': abstract_entity_indices,\n",
    "        }\n",
    "\n",
    "        category = row.get('category', None)\n",
    "        if pd.notnull(category):\n",
    "            newsid2cat[nid] = category\n",
    "            category_news_dict[category].append(nid)\n",
    "\n",
    "    return news_dict, newsid2cat, category_news_dict, cat2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Re1I2EwO1SLa"
   },
   "outputs": [],
   "source": [
    "def build_news_freq_dict(behaviors_df):\n",
    "    news_freq_counter = Counter()\n",
    "    for history in behaviors_df['history'].dropna():\n",
    "        for nid in history.split():\n",
    "            news_freq_counter[nid] += 1\n",
    "    return dict(news_freq_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Us1JcN6uI0_7"
   },
   "source": [
    "#Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "dqT9Zfx-I8sU"
   },
   "outputs": [],
   "source": [
    "def clicked_candidate_news_preparation(behaviors, news):\n",
    "    history_field = behaviors['history']\n",
    "    impressions_field = behaviors['impressions']\n",
    "\n",
    "    history_ids = history_field.split() if isinstance(history_field, str) and history_field.strip() else []\n",
    "    impression_pairs = impressions_field.split() if isinstance(impressions_field, str) and impressions_field.strip() else []\n",
    "\n",
    "    candidate_ids = [impr.split('-')[0] for impr in impression_pairs]\n",
    "    candidate_labels = [int(impr.split('-')[1]) for impr in impression_pairs]\n",
    "\n",
    "    clicked_news = [nid for nid in history_ids if nid in set(news['id'])]\n",
    "    candidate_news = [nid for nid in candidate_ids if nid in set(news['id'])]\n",
    "\n",
    "    return clicked_news, candidate_news, candidate_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "HUMCa14liawj"
   },
   "outputs": [],
   "source": [
    "def prepare_news_tensor(news_ids, news_dict, device):\n",
    "    tensor = torch.tensor(\n",
    "        [news_dict[nid]['title_indices'] for nid in news_ids],\n",
    "        dtype=torch.long\n",
    "    )\n",
    "    return tensor.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HsrmmTNcqPY"
   },
   "source": [
    "#Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wzjz8O5cdCZ5"
   },
   "source": [
    "##Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "TbdXAh8vc_8O"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Vl3AstLY0l6"
   },
   "source": [
    "##ChannelWiseLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "naXmu0AxYvhS"
   },
   "outputs": [],
   "source": [
    "class ChannelWiseDynamicConv(nn.Module):\n",
    "    def __init__(self, category_size, category_emb_dim=300, word_dim=300, kernel_size=3):\n",
    "        super(ChannelWiseDynamicConv, self).__init__()\n",
    "        self.category_embedding = nn.Embedding(category_size, category_emb_dim)\n",
    "        self.kernel_generator = nn.Sequential(\n",
    "            nn.Linear(category_emb_dim, word_dim * kernel_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.kernel_size = kernel_size\n",
    "        self.word_dim = word_dim\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, word_vecs):\n",
    "        B, L, D = word_vecs.shape\n",
    "        k = self.kernel_size\n",
    "        device = word_vecs.device\n",
    "\n",
    "\n",
    "        C_size = self.category_embedding.num_embeddings\n",
    "        C = self.category_embedding(torch.arange(C_size, device=device))\n",
    "\n",
    "        Kc = self.kernel_generator(C)\n",
    "        Kc = Kc.view(C_size, D, k)\n",
    "\n",
    "\n",
    "        W = torch.matmul(word_vecs, C.T)\n",
    "        W = F.softmax(W, dim=-1)\n",
    "\n",
    "        Ki = torch.matmul(W, Kc.view(C_size, -1))\n",
    "        Ki = Ki.view(B, L, D, k)\n",
    "\n",
    "        mi = word_vecs.transpose(1, 2)\n",
    "        mi = F.pad(mi, (k // 2, k // 2), mode='replicate')\n",
    "\n",
    "        outputs = torch.zeros(B, L, D, device=device)\n",
    "        for i in range(L):\n",
    "            x_slice = mi[:, :, i:i + k]\n",
    "            Ki_sample = Ki[:, i, :, :]\n",
    "            outputs[:, i, :] = torch.sum(x_slice * Ki_sample, dim=-1)\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bi1HgGSEcyvu"
   },
   "source": [
    "##News Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ccezVPD1ZSdZ"
   },
   "outputs": [],
   "source": [
    "class NewsEncoder(nn.Module):\n",
    "    def __init__(self, embedding_matrix, category_size, word_dim=300, num_heads=10, kernel_size=3, dropout=0.2):\n",
    "        super(NewsEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))\n",
    "        self.multihead_attention = nn.MultiheadAttention(word_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(word_dim, word_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(word_dim, word_dim)\n",
    "        )\n",
    "        self.dynamic_conv = ChannelWiseDynamicConv(\n",
    "            category_size=category_size,\n",
    "            word_dim=word_dim,\n",
    "            kernel_size=kernel_size\n",
    "        )\n",
    "        self.query_vector = nn.Parameter(torch.randn(word_dim))\n",
    "        self.norm1 = nn.LayerNorm(word_dim)\n",
    "        self.norm2 = nn.LayerNorm(word_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, word_indices):\n",
    "        emb = self.embedding(word_indices)\n",
    "        hi, _ = self.multihead_attention(emb, emb, emb)\n",
    "        hi = self.dropout(hi)\n",
    "        hi_norm = self.norm1(emb + hi)\n",
    "\n",
    "        ffn_out = self.feed_forward(hi_norm)\n",
    "        ffn_out = self.dropout(ffn_out)\n",
    "        ffn_out_norm = self.norm2(hi_norm + ffn_out)\n",
    "\n",
    "        dyn_out = self.dynamic_conv(ffn_out_norm)\n",
    "        final_word_repr = ffn_out_norm + dyn_out\n",
    "\n",
    "        attn_scores = torch.matmul(final_word_repr, self.query_vector)\n",
    "        attn_weights = F.softmax(attn_scores, dim=1).unsqueeze(-1)\n",
    "        news_vec = torch.sum(final_word_repr * attn_weights, dim=1)\n",
    "\n",
    "        return news_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlNzo0_M-NUp"
   },
   "source": [
    "##User Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "FsC5e0aKtlsl"
   },
   "outputs": [],
   "source": [
    "class UserEncoder(nn.Module):\n",
    "    def __init__(self, news_dim=300):\n",
    "        super(UserEncoder, self).__init__()\n",
    "        self.V = nn.Linear(news_dim, news_dim)\n",
    "        self.query_vector = nn.Parameter(torch.randn(news_dim))\n",
    "\n",
    "    def forward(self, clicked_news_vecs):\n",
    "        if clicked_news_vecs.dim() == 2:\n",
    "            clicked_news_vecs = clicked_news_vecs.unsqueeze(0)\n",
    "\n",
    "        atten = torch.tanh(self.V(clicked_news_vecs.clone()))\n",
    "        ai = torch.matmul(atten, self.query_vector.clone())\n",
    "        ai = F.softmax(ai, dim=1).unsqueeze(-1)\n",
    "        user_vec = torch.sum(ai * clicked_news_vecs, dim=1)\n",
    "\n",
    "        return user_vec.squeeze(0) if user_vec.size(0) == 1 else user_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3YTOKDjLeZb"
   },
   "source": [
    "##Frequency-Aware Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "vMD6cpUlrhMy"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "51CaymkCLcuC"
   },
   "outputs": [],
   "source": [
    "def dropoutORreplace(clicked_ids, news_freq_dict, category_news_dict, newsid2cat, r_min=0.1, r_max=0.4, method='dropout'):\n",
    "    all_freqs = [math.log(news_freq_dict.get(nid, 1)) for user in clicked_ids for nid in user]\n",
    "    max_freq = max(all_freqs) if all_freqs else 1\n",
    "    if max_freq == 0:\n",
    "        max_freq = 1e-6\n",
    "\n",
    "    modified_batch = []\n",
    "\n",
    "    for user_clicks in clicked_ids:\n",
    "        modified = []\n",
    "        for nid in user_clicks:\n",
    "            freq = math.log(news_freq_dict.get(nid, 1))\n",
    "            r_i = (freq / max_freq) * (r_max - r_min) + r_min\n",
    "\n",
    "            if method == 'dropout':\n",
    "                if random.random() < r_i:\n",
    "                    continue\n",
    "                modified.append(nid)\n",
    "\n",
    "            elif method == 'replace':\n",
    "                if random.random() < r_i:\n",
    "                    cat = newsid2cat.get(nid, None)\n",
    "                    pool = category_news_dict.get(cat, [])\n",
    "                    if pool:\n",
    "                        replacement = random.choice(pool)\n",
    "                        modified.append(replacement)\n",
    "                    else:\n",
    "                        modified.append(nid)\n",
    "                else:\n",
    "                    modified.append(nid)\n",
    "        modified_batch.append(modified)\n",
    "\n",
    "    return modified_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67Lkd6PMDG1q"
   },
   "source": [
    "#Score Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "-JXalL7PxVrB"
   },
   "outputs": [],
   "source": [
    "def compute_click_scores(user_vec, candidate_vecs):\n",
    "    click_scores = torch.matmul(candidate_vecs, user_vec)\n",
    "    return click_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8oteKukBdi7a"
   },
   "source": [
    "#Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "5CI4nnb_dc5u"
   },
   "outputs": [],
   "source": [
    "def contrastive_user_loss(v_orig, v_aug, temperature=0.1):\n",
    "    if v_orig.dim() == 1:\n",
    "        v_orig = v_orig.unsqueeze(0)\n",
    "    if v_aug.dim() == 1:\n",
    "        v_aug = v_aug.unsqueeze(0)\n",
    "\n",
    "    v_orig = F.normalize(v_orig, dim=1)\n",
    "    v_aug = F.normalize(v_aug, dim=1)\n",
    "\n",
    "    pos_sim = torch.exp(torch.sum(v_orig * v_aug, dim=1) / temperature)\n",
    "    denom = pos_sim + torch.exp(torch.sum(v_orig * v_orig, dim=1) / temperature)\n",
    "\n",
    "    loss = -torch.log(pos_sim / denom)\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xofUHpwHq9T"
   },
   "source": [
    "#Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "enbeVBeeIK8V"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "TSeicvuMWr4z"
   },
   "outputs": [],
   "source": [
    "def compute_mrr(y_true_sorted):\n",
    "    for i, label in enumerate(y_true_sorted):\n",
    "        if label == 1:\n",
    "            return 1 / (i + 1)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "oDwRUPfCHw4c"
   },
   "outputs": [],
   "source": [
    "def ndcg(y_true_sorted, k):\n",
    "    k = min(k, len(y_true_sorted))\n",
    "    if k == 0:\n",
    "        return 0.0\n",
    "    gains = (2 ** y_true_sorted[:k] - 1)\n",
    "    discounts = torch.log2(torch.arange(2, k + 2, device=y_true_sorted.device))\n",
    "    dcg = torch.sum(gains / discounts)\n",
    "\n",
    "    sorted_relevances, _ = torch.sort(y_true_sorted, descending=True)\n",
    "    ideal_gains = (2 ** sorted_relevances[:k] - 1)\n",
    "    ideal_dcg = torch.sum(ideal_gains / discounts)\n",
    "\n",
    "    return (dcg / ideal_dcg).item() if ideal_dcg > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "_9l9CWpsITtZ"
   },
   "outputs": [],
   "source": [
    "def eval_metrics(y_true, y_score):\n",
    "    aucs, mrrs, ndcg5s, ndcg10s = [], [], [], []\n",
    "\n",
    "    for labels, scores in zip(y_true, y_score):\n",
    "        sorted_idx = torch.argsort(scores, descending=True)\n",
    "        sorted_labels = labels[sorted_idx]\n",
    "\n",
    "        auc = roc_auc_score(labels.numpy(), scores.numpy()) if len(set(labels.numpy())) > 1 else 0.0\n",
    "        mrr = compute_mrr(sorted_labels)\n",
    "        ndcg5 = ndcg(sorted_labels, 5)\n",
    "        ndcg10 = ndcg(sorted_labels, 10)\n",
    "\n",
    "        aucs.append(auc)\n",
    "        mrrs.append(mrr)\n",
    "        ndcg5s.append(ndcg5)\n",
    "        ndcg10s.append(ndcg10)\n",
    "\n",
    "    auc = sum(aucs) / len(aucs)\n",
    "    mrr = sum(mrrs) / len(mrrs)\n",
    "    ndcg5 = sum(ndcg5s) / len(ndcg5s)\n",
    "    ndcg10 = sum(ndcg10s) / len(ndcg10s)\n",
    "\n",
    "    return auc, mrr, ndcg5, ndcg10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93jlgWByllIO"
   },
   "source": [
    "#Model Object Initiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 281784,
     "status": "ok",
     "timestamp": 1746444528747,
     "user": {
      "displayName": "Zannatun Sristy",
      "userId": "17694043809177177827"
     },
     "user_tz": 300
    },
    "id": "7nKwbUheSgwx",
    "outputId": "4acd278f-6df7-43e9-cd66-82406997634a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [01:44, 20998.87it/s]\n"
     ]
    }
   ],
   "source": [
    "glove = load_glove_embeddings('glove.840B.300d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qksg_-8XvcoA"
   },
   "source": [
    "##For training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "DJUjEmXPvYgc"
   },
   "outputs": [],
   "source": [
    "vocab = build_vocab(Train_News_data, glove)\n",
    "embedding_matrix = create_embedding_matrix(vocab, glove)\n",
    "news_dict, newsid2cat, category_news_dict, cat2idx = build_news_dict(Train_News_data, vocab)\n",
    "category_size = len(cat2idx)\n",
    "news_freq = build_news_freq_dict(Train_Behaviors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvcRYNHoviMY"
   },
   "source": [
    "##For testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "RYSFNQhUvm4z"
   },
   "outputs": [],
   "source": [
    "news_dict_val, newsid2cat_val, category_news_dict_val, cat2idx_val = build_news_dict(Val_News_data, vocab)\n",
    "category_size_val = len(cat2idx_val)\n",
    "news_freq_val = build_news_freq_dict(Val_Behaviors_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1746447078259,
     "user": {
      "displayName": "Zannatun Sristy",
      "userId": "17694043809177177827"
     },
     "user_tz": 300
    },
    "id": "TfVswNnelhxb",
    "outputId": "7dfdf4da-efb3-4a35-ff1a-d7ece9540648"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_842612/2111804828.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))\n"
     ]
    }
   ],
   "source": [
    "news_encoder = NewsEncoder(embedding_matrix, category_size)\n",
    "user_encoder = UserEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "ApAYK_-lDYh0"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1746447080592,
     "user": {
      "displayName": "Zannatun Sristy",
      "userId": "17694043809177177827"
     },
     "user_tz": 300
    },
    "id": "sc9HMFYAlzPw",
    "outputId": "06b9ac30-8ad4-4d4b-8150-2e83992316ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UserEncoder(\n",
       "  (V): Linear(in_features=300, out_features=300, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_encoder.to(device)\n",
    "user_encoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDr8Tq9djsYY"
   },
   "source": [
    "#Code Component Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 712,
     "status": "ok",
     "timestamp": 1746444531805,
     "user": {
      "displayName": "Zannatun Sristy",
      "userId": "17694043809177177827"
     },
     "user_tz": 300
    },
    "id": "zOV9c16sjH4U",
    "outputId": "42b787c6-647f-4d7e-cc3b-49288d1a411c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.7967, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "behavior_row = Train_Behaviors_data.iloc[1]\n",
    "clicked_news, candidate_news, candidate_labels = clicked_candidate_news_preparation(behavior_row, Train_News_data)\n",
    "\n",
    "# print(clicked_news)\n",
    "\n",
    "modified_clicked_news = dropoutORreplace([clicked_news], news_freq, category_news_dict, newsid2cat, r_min=0.1, r_max=0.4, method='dropout')[0]\n",
    "\n",
    "# print(modified_clicked_news)\n",
    "\n",
    "clicked_tensor = prepare_news_tensor(clicked_news, news_dict, device)\n",
    "candidate_tensor = prepare_news_tensor(candidate_news, news_dict, device)\n",
    "modified_clicked_tensor = prepare_news_tensor(modified_clicked_news, news_dict, device)\n",
    "\n",
    "# print(clicked_tensor)\n",
    "# print(modified_clicked_tensor)\n",
    "\n",
    "final_clicked_vecs = news_encoder(clicked_tensor)\n",
    "final_cand_vecs = news_encoder(candidate_tensor)\n",
    "final_mod_clicked_vecs = news_encoder(modified_clicked_tensor)\n",
    "\n",
    "# print(final_clicked_vecs)\n",
    "# print(final_mod_clicked_vecs)\n",
    "\n",
    "\n",
    "user_vec = user_encoder(final_clicked_vecs.unsqueeze(0))\n",
    "user_vec_aug = user_encoder(final_mod_clicked_vecs.unsqueeze(0))\n",
    "\n",
    "contrastive_loss = contrastive_user_loss(\n",
    "        user_vec, user_vec_aug, temperature=0.1\n",
    "        )\n",
    "\n",
    "print(contrastive_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AklbZifIsh3T"
   },
   "source": [
    "#Train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1746450575760,
     "user": {
      "displayName": "Zannatun Sristy",
      "userId": "17694043809177177827"
     },
     "user_tz": 300
    },
    "id": "otCufHBNRFia"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(behaviors_df, news_df, model_components, optimizer, news_dict, news_freq, category_news_dict, newsid2cat, device):\n",
    "    news_encoder = model_components['news_encoder']\n",
    "    user_encoder = model_components['user_encoder']\n",
    "\n",
    "    news_encoder.train()\n",
    "    user_encoder.train()\n",
    "\n",
    "\n",
    "    device = next(news_encoder.parameters()).device\n",
    "    total_loss = 0\n",
    "    contrastive_loss_total = 0\n",
    "    count = 0\n",
    "\n",
    "    user_vecs = []\n",
    "    user_vecs_aug = []\n",
    "\n",
    "    for _, behavior_row in tqdm(behaviors_df.iterrows(), total=len(behaviors_df)):\n",
    "        clicked_news, candidate_news, candidate_labels = clicked_candidate_news_preparation(behavior_row, news_df)\n",
    "        clicked_news = clicked_news[:50]\n",
    "\n",
    "        if len(candidate_labels) == 0 or len(clicked_news) == 0 or len(candidate_news) == 0:\n",
    "            continue\n",
    "\n",
    "        modified_clicked_news = dropoutORreplace([clicked_news], news_freq, category_news_dict, newsid2cat, r_min=0.1, r_max=0.4, method='dropout')[0]\n",
    "\n",
    "        if len(modified_clicked_news) == 0:\n",
    "            continue\n",
    "\n",
    "\n",
    "        clicked_tensor = prepare_news_tensor(clicked_news, news_dict, device)\n",
    "        mod_clicked_tensor = prepare_news_tensor(modified_clicked_news, news_dict, device)\n",
    "        candidate_tensor = prepare_news_tensor(candidate_news, news_dict, device)\n",
    "        candidate_labels = torch.tensor(candidate_labels, dtype=torch.float).to(device)\n",
    "\n",
    "\n",
    "        clicked_vecs = news_encoder(clicked_tensor)\n",
    "        mod_clicked_vecs = news_encoder(mod_clicked_tensor)\n",
    "        candidate_vecs = news_encoder(candidate_tensor)\n",
    "\n",
    "\n",
    "\n",
    "        user_vec = user_encoder(clicked_vecs.unsqueeze(0))\n",
    "        user_vec_aug = user_encoder(mod_clicked_vecs.unsqueeze(0))\n",
    "\n",
    "\n",
    "        scores = compute_click_scores(user_vec, candidate_vecs)\n",
    "        click_loss = F.binary_cross_entropy_with_logits(scores, candidate_labels)\n",
    "\n",
    "        contrastive_loss = contrastive_user_loss(\n",
    "        user_vec, user_vec_aug, temperature=0.1\n",
    "        )\n",
    "\n",
    "        loss = click_loss + 0.1 * contrastive_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += click_loss\n",
    "        contrastive_loss_total += contrastive_loss\n",
    "        count += 1\n",
    "\n",
    "    avg_click_loss = total_loss / count if count > 0 else 0\n",
    "    avg_contrastive_loss = contrastive_loss_total / count if count > 0 else 0\n",
    "    return avg_click_loss, avg_contrastive_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1746450578469,
     "user": {
      "displayName": "Zannatun Sristy",
      "userId": "17694043809177177827"
     },
     "user_tz": 300
    },
    "id": "zqIPl78dYA7d"
   },
   "outputs": [],
   "source": [
    "def test_one_epoch(behaviors_df, news_df, model_components, news_dict, news_freq, category_news_dict, newsid2cat, device):\n",
    "    news_encoder = model_components['news_encoder']\n",
    "    user_encoder = model_components['user_encoder']\n",
    "\n",
    "    news_encoder.eval()\n",
    "    user_encoder.eval()\n",
    "\n",
    "    device = next(news_encoder.parameters()).device\n",
    "    all_scores, all_labels = [], []\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for _, behavior_row in tqdm(behaviors_df.iterrows(), total=len(behaviors_df)):\n",
    "          clicked_news, candidate_news, candidate_labels = clicked_candidate_news_preparation(behavior_row, news_df)\n",
    "          clicked_news = clicked_news[:50]\n",
    "\n",
    "          if len(candidate_labels) == 0 or len(clicked_news) == 0 or len(candidate_news) == 0:\n",
    "              continue\n",
    "\n",
    "          modified_clicked_news = dropoutORreplace([clicked_news], news_freq, category_news_dict, newsid2cat, r_min=0.1, r_max=0.4, method='dropout')[0]\n",
    "\n",
    "          if len(modified_clicked_news) == 0:\n",
    "              continue\n",
    "\n",
    "\n",
    "          clicked_tensor = prepare_news_tensor(clicked_news, news_dict, device)\n",
    "          mod_clicked_tensor = prepare_news_tensor(modified_clicked_news, news_dict, device)\n",
    "          candidate_tensor = prepare_news_tensor(candidate_news, news_dict, device)\n",
    "          candidate_labels = torch.tensor(candidate_labels, dtype=torch.float).to(device)\n",
    "\n",
    "\n",
    "          clicked_vecs = news_encoder(clicked_tensor)\n",
    "          mod_clicked_vecs = news_encoder(mod_clicked_tensor)\n",
    "          candidate_vecs = news_encoder(candidate_tensor)\n",
    "\n",
    "\n",
    "\n",
    "          user_vec = user_encoder(clicked_vecs.unsqueeze(0))\n",
    "          user_vec_aug = user_encoder(mod_clicked_vecs.unsqueeze(0))\n",
    "\n",
    "\n",
    "          click_scores = compute_click_scores(user_vec, candidate_vecs)\n",
    "\n",
    "\n",
    "          all_scores.append(click_scores.detach().cpu())\n",
    "          all_labels.append(torch.tensor(candidate_labels, dtype=torch.float32))\n",
    "\n",
    "      return eval_metrics(all_labels, all_scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "gN8u5XVO8t53"
   },
   "outputs": [],
   "source": [
    "model_components = {\n",
    "    'news_encoder': news_encoder,\n",
    "    'user_encoder': user_encoder\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "Ee9VPoEF8y7q"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    list(news_encoder.parameters()) +\n",
    "    list(user_encoder.parameters()), lr=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1746447094323,
     "user": {
      "displayName": "Zannatun Sristy",
      "userId": "17694043809177177827"
     },
     "user_tz": 300
    },
    "id": "MwbC7GZJ1Tde",
    "outputId": "a5ea698f-64b4-4ede-d823-97f64d94477c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f1d1da661a0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 740394,
     "status": "ok",
     "timestamp": 1746451322433,
     "user": {
      "displayName": "Zannatun Sristy",
      "userId": "17694043809177177827"
     },
     "user_tz": 300
    },
    "id": "WfoO775VjBcY",
    "outputId": "03afd0e1-bfe9-4faf-e978-dff8fe6c0dd3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 1789/15569 [15:33<1:59:52,  1.92it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_loss, contrastive_loss_total \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m         \u001b[49m\u001b[43mTrain_Behaviors_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTrain_News_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnews_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnews_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategory_news_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewsid2cat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m auc, mrr, ndcg5, ndcg10 \u001b[38;5;241m=\u001b[39m test_one_epoch(\n\u001b[1;32m      6\u001b[0m     Val_Behaviors_data, Val_News_data, model_components, news_dict_val, news_freq_val, category_news_dict_val, newsid2cat_val, device\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Contrastive Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontrastive_loss_total\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, MRR: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmrr\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, nDCG@5: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndcg5\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, nDCG@10: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndcg10\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[35], line 56\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(behaviors_df, news_df, model_components, optimizer, news_dict, news_freq, category_news_dict, newsid2cat, device)\u001b[0m\n\u001b[1;32m     53\u001b[0m loss \u001b[38;5;241m=\u001b[39m click_loss \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m contrastive_loss\n\u001b[1;32m     55\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 56\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     59\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m click_loss\n",
      "File \u001b[0;32m/common/software/install/migrated/anaconda/python3-2023.03-libmamba/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/common/software/install/migrated/anaconda/python3-2023.03-libmamba/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss, contrastive_loss_total = train_one_epoch(\n",
    "         Train_Behaviors_data, Train_News_data, model_components, optimizer, news_dict, news_freq, category_news_dict, newsid2cat, device\n",
    "    )\n",
    "\n",
    "auc, mrr, ndcg5, ndcg10 = test_one_epoch(\n",
    "    Val_Behaviors_data, Val_News_data, model_components, news_dict_val, news_freq_val, category_news_dict_val, newsid2cat_val, device\n",
    ")\n",
    "\n",
    "print(f\" Train Loss: {train_loss:.4f} | Contrastive Loss: {contrastive_loss_total}\"\n",
    "      f\"AUC: {auc:.4f}, MRR: {mrr:.4f}, nDCG@5: {ndcg5:.4f}, nDCG@10: {ndcg10:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5lEOl4bYF56"
   },
   "source": [
    "#Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 911
    },
    "executionInfo": {
     "elapsed": 165389,
     "status": "error",
     "timestamp": 1746450218017,
     "user": {
      "displayName": "Zannatun Sristy",
      "userId": "17694043809177177827"
     },
     "user_tz": 300
    },
    "id": "P5ht49mL8qzu",
    "outputId": "70f09f58-8eab-4f96-b49b-e2648569fdf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15698/15698 [2:17:10<00:00,  1.91it/s]   \n",
      "  0%|          | 0/7378 [00:00<?, ?it/s]/tmp/ipykernel_842612/4072907153.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  all_labels.append(torch.tensor(candidate_labels, dtype=torch.float32))\n",
      "100%|██████████| 7378/7378 [13:43<00:00,  8.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train Loss: 14.4284 | AUC: 0.5382, MRR: 0.2761, nDCG@5: 0.2535, nDCG@10: 0.3134\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15698/15698 [2:17:35<00:00,  1.90it/s]  \n",
      "  0%|          | 0/7378 [00:00<?, ?it/s]/tmp/ipykernel_842612/4072907153.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  all_labels.append(torch.tensor(candidate_labels, dtype=torch.float32))\n",
      "100%|██████████| 7378/7378 [14:23<00:00,  8.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 8.2332 | AUC: 0.5112, MRR: 0.2573, nDCG@5: 0.2322, nDCG@10: 0.2964\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15698/15698 [2:23:05<00:00,  1.83it/s]   \n",
      "  0%|          | 0/7378 [00:00<?, ?it/s]/tmp/ipykernel_842612/4072907153.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  all_labels.append(torch.tensor(candidate_labels, dtype=torch.float32))\n",
      "100%|██████████| 7378/7378 [14:41<00:00,  8.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 6.2969 | AUC: 0.5326, MRR: 0.2697, nDCG@5: 0.2468, nDCG@10: 0.3070\n",
      "-------------------------------------------Final Eval-------------------------------------------\n",
      "AUC: 53.26, MRR: 26.97, nDCG@5: 24.68, nDCG@10: 30.70\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "\n",
    "\n",
    "    train_loss, contrastive_loss_total = train_one_epoch(\n",
    "         Train_Behaviors_data, Train_News_data, model_components, optimizer, news_dict, news_freq, category_news_dict, newsid2cat, device\n",
    "    )\n",
    "\n",
    "    auc, mrr, ndcg5, ndcg10 = test_one_epoch(\n",
    "        Val_Behaviors_data, Val_News_data, model_components, news_dict_val, news_freq_val, category_news_dict_val, newsid2cat_val, device\n",
    "    )\n",
    "\n",
    "    print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f} | \"\n",
    "          f\"AUC: {auc:.4f}, MRR: {mrr:.4f}, nDCG@5: {ndcg5:.4f}, nDCG@10: {ndcg10:.4f}\")\n",
    "\n",
    "print(\"-------------------------------------------Final Eval-------------------------------------------\")\n",
    "print(f\"AUC: {auc * 100:.2f}, MRR: {mrr * 100:.2f}, nDCG@5: {ndcg5 * 100:.2f}, nDCG@10: {ndcg10 * 100:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_name = \"MCCM\"\n",
    "\n",
    "df = pd.DataFrame([{\n",
    "    'Module': module_name,\n",
    "    'AUC': f\"{auc * 100:.2f}\",\n",
    "    'MRR': f\"{mrr * 100:.2f}\",\n",
    "    'nDCG@5': f\"{ndcg5 * 100:.2f}\",\n",
    "    'nDCG@10': f\"{ndcg10 * 100:.2f}\",\n",
    "}])\n",
    "\n",
    "filename = f\"{module_name}_results.csv\"\n",
    "\n",
    "df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python3.10",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
